{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "use_cuda  True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from archived.seq2seq.model_seq2seq import *\n",
    "from utils import Doc, read\n",
    "from attribute import TFIDFKeywordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dev_lyrics = read()[:500]\n",
    "# Doc.load_corpus(dev_lyrics)\n",
    "Doc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Doc.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10003\n",
      "10000\n",
      "10001\n",
      "10002\n",
      "PAD\n"
     ]
    }
   ],
   "source": [
    "symbols = ['PAD', 'SOS', 'EOS']\n",
    "Doc.extend_vocab(symbols)\n",
    "vocab = Doc.get_vocab()\n",
    "print(len(vocab))\n",
    "for symbol in symbols:\n",
    "    print(vocab[symbol])\n",
    "print(Doc.idxs_to_text(vocab['PAD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Doc.text_to_idxs(\"EOS_token\")\n",
    "# len(Doc.get_vocab())\n",
    "# Doc.text_to_idxs(\"一万\")\n",
    "# Doc.idxs_to_text(np.array([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# special_words = {\n",
    "#     \"SOS_token\": 0,\n",
    "#     \"EOS_token\": 1,\n",
    "#     \"PAD_token\": 2\n",
    "#                 }\n",
    "# # a = Doc.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_list_of_lines = [doc.get_lines() for doc in Doc.get_corpus()]\n",
    "# e = TFIDFKeywordExtractor(list_of_list_of_lines, len(Doc.get_vocab()))\n",
    "#e.get_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sample = list_of_list_of_lines\n",
    "# print(Doc.idxs_to_text(text_sample[0]))\n",
    "keywords = TFIDFKeywordExtractor(text_sample, len(Doc.get_vocab()),n_keywords = 1)\n",
    "keywords.get_keywords()\n",
    "pairs = list(zip([i for j in keywords.get_keywords() for i in j],\\\n",
    "            [i for j in text_sample for i in j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "teacher_forcing_ratio = 0.5\n",
    "MIN_LENGTH = 0\n",
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 32\n",
    "# SOS_token = 0\n",
    "# EOS_token = 1\n",
    "# PAD_token = 2\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def keywords_filterPair(p):\n",
    "    return MIN_LENGTH < len(p[0]) < MAX_LENGTH and \\\n",
    "        MIN_LENGTH < len(p[1]) < MAX_LENGTH\n",
    "\n",
    "def keywords_filterPairs(pairs):\n",
    "    return [pair for pair in pairs if keywords_filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_pairs = keywords_filterPairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinxingyu/anaconda2/envs/lyrics/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_input,val_input, _, __ =  \\\n",
    "    train_test_split(filtered_pairs,np.zeros(np.array(filtered_pairs).shape),test_size=0.2, random_state=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def earlystopping(loss_list, patience):\n",
    "    if len(loss_list)> patience:\n",
    "        loss_diff = np.array(loss_list[1:]) - np.array(loss_list[:-1])\n",
    "        if loss_diff[(-1)*patience:].all() > 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder,train_input, val_input,vocab, n_iters = 2000, patience = 10, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_train_losses = []\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    plot_val_losses = []\n",
    "    print_val_loss_total = 0  # Reset every print_every\n",
    "    plot_val_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pairs = [keywords_variablesFromPair(random.choice(train_input)) for i in range(BATCH_SIZE)]\n",
    "        train_input_variable = torch.cat([pair[0] for pair in training_pairs], 1)\n",
    "        train_target_variable = torch.cat([pair[1] for pair in training_pairs], 1)\n",
    "        \n",
    "        train_loss = train(train_input_variable, train_target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, vocab)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "\n",
    "        val_pairs = [keywords_variablesFromPair(random.choice(val_input)) for i in range(BATCH_SIZE)]\n",
    "        val_input_variable = torch.cat([pair[0] for pair in val_pairs], 1)\n",
    "        val_target_variable = torch.cat([pair[1] for pair in val_pairs], 1)\n",
    "        val_loss = evaluate(val_input_variable, val_target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, vocab)\n",
    "        print_val_loss_total += val_loss\n",
    "        plot_val_loss_total += val_loss\n",
    "        \n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_train_loss_avg))\n",
    "            print_val_loss_avg = print_val_loss_total / print_every\n",
    "            print_val_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_val_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_val_loss_avg = plot_val_loss_total / plot_every\n",
    "            plot_val_losses.append(plot_val_loss_avg)\n",
    "            plot_val_loss_total = 0\n",
    "            if earlystopping(plot_val_losses, patience) == True:\n",
    "                print(\"Training process stopped due to earlystopping with patience \", patience)\n",
    "                break\n",
    "            predictRandomly(encoder, decoder, val_input, vocab)\n",
    "            showPlot(plot_train_losses,plot_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywords_variableFromIndex(indexes, reverse=False):\n",
    "    indexes = indexes.tolist()\n",
    "    indexes.append(vocab['EOS'])\n",
    "    indexes = [vocab['PAD']] * (MAX_LENGTH - len(indexes)) + indexes\n",
    "    if reverse:\n",
    "        indexes = indexes[::-1]\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def keywords_variablesFromPair(pair):\n",
    "    input_variable = keywords_variableFromIndex(pair[0])\n",
    "    target_variable = keywords_variableFromIndex(pair[1], False)\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points1, points2 = []):\n",
    "#     plt.figure()\n",
    "#     fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "#     loc = ticker.MultipleLocator(base=0.2)\n",
    "#     ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points1)\n",
    "    plt.plot(points2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, input_variable,vocab, max_length=MAX_LENGTH, beam_width=30):\n",
    "    input_length = len(input_variable)\n",
    "    encoder_hidden = encoder.initHidden(beam_width)\n",
    "\n",
    "    input_variable = torch.cat([input_variable for i in range(beam_width)], 1)\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(beam_width, max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[:, ei] = encoder_output[0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[vocab['SOS']]] * beam_width))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_hidden_copy = Variable(torch.zeros(encoder1.n_layers, beam_width, hidden_size))\n",
    "    decoder_hidden_copy = decoder_hidden_copy.cuda() if use_cuda else decoder_hidden_copy\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    probability = torch.zeros(beam_width).view(-1, 1)\n",
    "    probability = probability.cuda() if use_cuda else probability\n",
    "    \n",
    "    prev = [[]] * max_length\n",
    "    idxs = [[]] * max_length\n",
    "    \n",
    "    cands = []\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "#       LogSoftmax\n",
    "        topv, topi = decoder_output.data.topk(beam_width)\n",
    "\n",
    "        if di == 0:\n",
    "            decoder_input = Variable(torch.LongTensor([[ni] for ni in topi[0]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            probability = topv[0].view(-1, 1)\n",
    "        else:\n",
    "#           Get beam_width candidate for each beam.\n",
    "            topv = topv + probability\n",
    "            topv, topi = topv.view(-1), topi.view(-1)\n",
    "\n",
    "#           Select beam_width from beam_width*beam_width.\n",
    "            _, topt = topv.topk(beam_width)\n",
    "\n",
    "#           Update adn prepare for the next step.\n",
    "            probability = topv[topt]\n",
    "            decoder_input = Variable(torch.LongTensor([[topi[topt[k]]] for k in range(beam_width)]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            prev[di] = [topt[k] // beam_width for k in range(beam_width)]\n",
    "            \n",
    "#           Don't forget to prepare the corresponding hidden state.\n",
    "            for i in range(beam_width):\n",
    "                decoder_hidden_copy[:, i] = decoder_hidden[:, prev[di][i]]\n",
    "            decoder_hidden = decoder_hidden_copy.clone()\n",
    "            \n",
    "#           If some beam meets its end.\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                if decoder_input.data[i, 0] == vocab['PAD'] and probability[i] > -np.inf:\n",
    "                    cands.append((probability[i], di, i))\n",
    "                    probability[i] = -np.inf\n",
    "\n",
    "#       Record each step's input\n",
    "        idxs[di] = decoder_input.data\n",
    "        \n",
    "    def full_sentence(start, di):\n",
    "        decoded_words = []\n",
    "        for loc in range(1, di+1)[::-1]:\n",
    "            #print([idxs[loc][start][0]])\n",
    "            #print(Doc.idxs_to_text(np.array([idxs[loc][start][0]])))\n",
    "            decoded_words = [Doc.idxs_to_text(np.array([idxs[loc][start][0]]))] + decoded_words\n",
    "            start = prev[loc][start]\n",
    "        return ''.join(decoded_words).replace('PAD', '')[::-1]\n",
    "\n",
    "    cands = sorted(cands)[::-1]\n",
    "\n",
    "    cands = list(map(lambda x: full_sentence(x[2], x[1]), cands))\n",
    "    \n",
    "    answers = []\n",
    "    for cand in cands:\n",
    "        if cand not in answers:\n",
    "            answers.append(cand)\n",
    "\n",
    "    return answers, decoder_attentions[:di + 1]        \n",
    "        \n",
    "\n",
    "def predictRandomly(encoder, decoder,pairs,vocab, n=5, beam_width=30, show=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', Doc.idxs_to_text(pair[0]))\n",
    "        print('=', Doc.idxs_to_text(pair[1]))\n",
    "        output_sentences, attentions = predict(encoder, decoder, keywords_variableFromIndex(pair[0]),vocab, beam_width=beam_width)\n",
    "        for j in range(min(len(output_sentences), show)):\n",
    "            print('<', output_sentences[j])\n",
    "        print()\n",
    "        \n",
    "def predict_results(keywords, encoder, decoder):\n",
    "    answers, _ = predict(encoder, decoder, keywords_variableFromIndex(keywords), max_length=MAX_LENGTH, beam_width=30)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict(Doc.text_to_idxs(np.array([\"梦想\",\"希望\",\"姑娘\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# encoder1 = EncoderRNN(vocab.n_words, hidden_size, n_layers=2)\n",
    "# decoder1 = DecoderRNN(hidden_size, vocab.n_words, n_layers=2)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, vocab.n_words, n_layers=2, dropout_p=0.1)\n",
    "\n",
    "# if use_cuda:\n",
    "#     encoder1 = encoder1.cuda()\n",
    "#     decoder1 = decoder1.cuda()\n",
    "#     attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "# encoder1.load_state_dict(torch.load('saved/encoder.params', map_location={'cuda:0': 'cpu'}))\n",
    "# attn_decoder1.load_state_dict(torch.load('saved/attn_decoder.params', map_location={'cuda:0': 'cpu'}))\n",
    "#trainIters(encoder1, attn_decoder1, 75000, print_every=10, plot_every=200, learning_rate=1e-3)\n",
    "# torch.save(encoder1.state_dict(), 'encoder.params')\n",
    "# torch.save(attn_decoder1.state_dict(), 'attn_decoder1.params')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#print('use_cuda ',use_cuda)\n",
    "\n",
    "encoder1 = EncoderRNN(len(Doc.get_vocab()), hidden_size, n_layers=2)\n",
    "decoder1 = DecoderRNN(hidden_size, len(Doc.get_vocab()), n_layers=2)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(Doc.get_vocab()), n_layers=2, dropout_p=0.1)\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "    \n",
    "# encoder1.load_state_dict(torch.load('saved/keywords_encoder.params', map_location={'cuda:0': 'cpu'}))\n",
    "# attn_decoder1.load_state_dict(torch.load('saved/keywords_attn_decoder.params', map_location={'cuda:0': 'cpu'}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 4s (- 716m 1s) (10 0%) 0.7185\n",
      "0m 4s (- 716m 3s) (10 0%) 0.8900\n",
      "0m 8s (- 708m 56s) (20 0%) 1.0052\n",
      "0m 8s (- 708m 57s) (20 0%) 0.8731\n",
      "0m 12s (- 698m 49s) (30 0%) 0.7724\n",
      "0m 12s (- 698m 50s) (30 0%) 0.9684\n",
      "0m 16s (- 706m 6s) (40 0%) 0.8133\n",
      "0m 16s (- 706m 6s) (40 0%) 1.0042\n",
      "0m 21s (- 702m 11s) (50 0%) 0.8065\n",
      "0m 21s (- 702m 11s) (50 0%) 0.8860\n",
      "0m 25s (- 698m 18s) (60 0%) 0.9160\n",
      "0m 25s (- 698m 18s) (60 0%) 0.8592\n",
      "0m 29s (- 696m 52s) (70 0%) 0.7981\n",
      "0m 29s (- 696m 53s) (70 0%) 1.0211\n",
      "0m 33s (- 695m 41s) (80 0%) 0.8692\n",
      "0m 33s (- 695m 41s) (80 0%) 0.9399\n",
      "0m 37s (- 697m 5s) (90 0%) 0.7886\n",
      "0m 37s (- 697m 5s) (90 0%) 0.8878\n",
      "0m 41s (- 696m 33s) (100 0%) 0.8471\n",
      "0m 41s (- 696m 33s) (100 0%) 0.8819\n",
      "0m 45s (- 695m 51s) (110 0%) 0.9104\n",
      "0m 45s (- 695m 52s) (110 0%) 1.0087\n",
      "0m 49s (- 693m 15s) (120 0%) 0.8321\n",
      "0m 49s (- 693m 15s) (120 0%) 0.8743\n",
      "0m 54s (- 693m 2s) (130 0%) 0.9166\n",
      "0m 54s (- 693m 2s) (130 0%) 0.9185\n",
      "0m 58s (- 695m 9s) (140 0%) 0.8606\n",
      "0m 58s (- 695m 9s) (140 0%) 0.8820\n",
      "1m 2s (- 693m 57s) (150 0%) 0.8354\n",
      "1m 2s (- 693m 57s) (150 0%) 0.9586\n",
      "1m 6s (- 691m 57s) (160 0%) 0.7574\n",
      "1m 6s (- 691m 57s) (160 0%) 0.8291\n",
      "1m 10s (- 693m 8s) (170 0%) 0.7489\n",
      "1m 10s (- 693m 8s) (170 0%) 0.9702\n",
      "1m 14s (- 692m 11s) (180 0%) 0.6831\n",
      "1m 14s (- 692m 11s) (180 0%) 0.9537\n",
      "1m 19s (- 693m 51s) (190 0%) 0.7273\n",
      "1m 19s (- 693m 51s) (190 0%) 0.9914\n",
      "1m 23s (- 692m 2s) (200 0%) 0.6220\n",
      "1m 23s (- 692m 2s) (200 0%) 0.7419\n",
      "> 浮现\n",
      "= 我承认爱情曾经浮现\n",
      "< \n",
      "< 我\n",
      "< 雨\n",
      "< 的你\n",
      "< 了\n",
      "\n",
      "> 眼\n",
      "= 我爱着你忘不了你的眼\n",
      "< \n",
      "< 遍走想\n",
      "< 死心\n",
      "< 在戒\n",
      "< 找想\n",
      "\n",
      "> 转\n",
      "= 转呀转呀转呀转呀\n",
      "< \n",
      "< 来想\n",
      "< 过心\n",
      "< 穿想\n",
      "< 分路\n",
      "\n",
      "> 死\n",
      "= 还不去死\n",
      "< \n",
      "< 了\n",
      "< 我为因\n",
      "< 我你\n",
      "< 啦啦\n",
      "\n",
      "> 那\n",
      "= 我爱你那么多所以那么痛\n",
      "< \n",
      "< 么我\n",
      "< 啦么\n",
      "< 了\n",
      "< 我你\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f872c072048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1,train_input, val_input,vocab, n_iters = 100000,patience = 1000, print_every=10, plot_every=200, learning_rate=1e-3)\n",
    "torch.save(encoder1.state_dict(), 'keywords_encoder.params')\n",
    "torch.save(decoder1.state_dict(), 'keywords_attn_decoder1.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {\n",
    "                        \"A\": -1,\n",
    "                        \"B\": -2,\n",
    "                        \"C\": -3\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in a.items():\n",
    "    a.update({k:v+len(a)})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict(Doc.text_to_idxs(np.array([\"梦想\"])))\n",
    "predict(encoder1,decoder1,Doc.text_to_idxs(\"梦想希望\"),vocab)\n",
    "#predict(Doc.text_to_idxs(),encoder1,attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lyrics]",
   "language": "python",
   "name": "conda-env-lyrics-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
