

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>gensim.models.word2vec &mdash; Deep Lyrics  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="Deep Lyrics  documentation" href="../../../index.html"/>
        <link rel="up" title="Module code" href="../../index.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> Deep Lyrics
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../utils.html">utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../topic.html">topic package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../preprocess.html">preprocess package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Deep Lyrics</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>gensim.models.word2vec</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for gensim.models.word2vec</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1">#</span>
<span class="c1"># Copyright (C) 2013 Radim Rehurek &lt;me@radimrehurek.com&gt;</span>
<span class="c1"># Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Produce word vectors with deep learning via word2vec&#39;s &quot;skip-gram and CBOW models&quot;, using either</span>
<span class="sd">hierarchical softmax or negative sampling [1]_ [2]_.</span>

<span class="sd">NOTE: There are more ways to get word vectors in Gensim than just Word2Vec. See wrappers for FastText, VarEmbed and WordRank.</span>

<span class="sd">The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/</span>
<span class="sd">and extended with additional functionality.</span>

<span class="sd">For a blog tutorial on gensim word2vec, with an interactive web app trained on GoogleNews, visit http://radimrehurek.com/2014/02/word2vec-tutorial/</span>

<span class="sd">**Make sure you have a C compiler before installing gensim, to use optimized (compiled) word2vec training**</span>
<span class="sd">(70x speedup compared to plain NumPy implementation [3]_).</span>

<span class="sd">Initialize a model with e.g.::</span>

<span class="sd">    &gt;&gt;&gt; model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)</span>

<span class="sd">Persist a model to disk with::</span>

<span class="sd">    &gt;&gt;&gt; model.save(fname)</span>
<span class="sd">    &gt;&gt;&gt; model = Word2Vec.load(fname)  # you can continue training with the loaded model!</span>

<span class="sd">The word vectors are stored in a KeyedVectors instance in model.wv. This separates the read-only word vector lookup operations in KeyedVectors from the training code in Word2Vec::</span>

<span class="sd">  &gt;&gt;&gt; model.wv[&#39;computer&#39;]  # numpy vector of a word</span>
<span class="sd">  array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)</span>

<span class="sd">The word vectors can also be instantiated from an existing file on disk in the word2vec C format as a KeyedVectors instance::</span>

<span class="sd">    NOTE: It is impossible to continue training the vectors loaded from the C format because hidden weights, vocabulary frequency and the binary tree is missing::</span>

<span class="sd">        &gt;&gt;&gt; from gensim.models.keyedvectors import KeyedVectors</span>
<span class="sd">        &gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(&#39;/tmp/vectors.txt&#39;, binary=False)  # C text format</span>
<span class="sd">        &gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(&#39;/tmp/vectors.bin&#39;, binary=True)  # C binary format</span>


<span class="sd">You can perform various NLP word tasks with the model. Some of them</span>
<span class="sd">are already built-in::</span>

<span class="sd">  &gt;&gt;&gt; model.wv.most_similar(positive=[&#39;woman&#39;, &#39;king&#39;], negative=[&#39;man&#39;])</span>
<span class="sd">  [(&#39;queen&#39;, 0.50882536), ...]</span>

<span class="sd">  &gt;&gt;&gt; model.wv.most_similar_cosmul(positive=[&#39;woman&#39;, &#39;king&#39;], negative=[&#39;man&#39;])</span>
<span class="sd">  [(&#39;queen&#39;, 0.71382287), ...]</span>


<span class="sd">  &gt;&gt;&gt; model.wv.doesnt_match(&quot;breakfast cereal dinner lunch&quot;.split())</span>
<span class="sd">  &#39;cereal&#39;</span>

<span class="sd">  &gt;&gt;&gt; model.wv.similarity(&#39;woman&#39;, &#39;man&#39;)</span>
<span class="sd">  0.73723527</span>

<span class="sd">Probability of a text under the model::</span>

<span class="sd">  &gt;&gt;&gt; model.score([&quot;The fox jumped over a lazy dog&quot;.split()])</span>
<span class="sd">  0.2158356</span>

<span class="sd">Correlation with human opinion on word similarity::</span>

<span class="sd">  &gt;&gt;&gt; model.wv.evaluate_word_pairs(os.path.join(module_path, &#39;test_data&#39;,&#39;wordsim353.tsv&#39;))</span>
<span class="sd">  0.51, 0.62, 0.13</span>

<span class="sd">And on analogies::</span>

<span class="sd">  &gt;&gt;&gt; model.wv.accuracy(os.path.join(module_path, &#39;test_data&#39;, &#39;questions-words.txt&#39;))</span>

<span class="sd">and so on.</span>

<span class="sd">If you&#39;re finished training a model (i.e. no more updates, only querying), then switch to the :mod:`gensim.models.KeyedVectors` instance in wv</span>

<span class="sd">  &gt;&gt;&gt; word_vectors = model.wv</span>
<span class="sd">  &gt;&gt;&gt; del model</span>

<span class="sd">to trim unneeded model memory = use much less RAM.</span>

<span class="sd">Note that there is a :mod:`gensim.models.phrases` module which lets you automatically</span>
<span class="sd">detect phrases longer than one word. Using phrases, you can learn a word2vec model</span>
<span class="sd">where &quot;words&quot; are actually multiword expressions, such as `new_york_times` or `financial_crisis`:</span>

<span class="sd">    &gt;&gt;&gt; bigram_transformer = gensim.models.Phrases(sentences)</span>
<span class="sd">    &gt;&gt;&gt; model = Word2Vec(bigram_transformer[sentences], size=100, ...)</span>

<span class="sd">.. [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.</span>
<span class="sd">.. [2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality.</span>
<span class="sd">       In Proceedings of NIPS, 2013.</span>
<span class="sd">.. [3] Optimizing word2vec in gensim, http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>  <span class="c1"># py3 &quot;true division&quot;</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">heapq</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="k">import</span> <span class="n">default_timer</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">gensim.utils</span> <span class="k">import</span> <span class="n">keep_vocab_item</span><span class="p">,</span> <span class="n">call_on_class_only</span>
<span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="k">import</span> <span class="n">KeyedVectors</span><span class="p">,</span> <span class="n">Vocab</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">queue</span> <span class="k">import</span> <span class="n">Queue</span><span class="p">,</span> <span class="n">Empty</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">Queue</span> <span class="k">import</span> <span class="n">Queue</span><span class="p">,</span> <span class="n">Empty</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">dot</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">outer</span><span class="p">,</span> <span class="n">random</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">float32</span> <span class="k">as</span> <span class="n">REAL</span><span class="p">,</span>\
    <span class="n">uint32</span><span class="p">,</span> <span class="n">seterr</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="n">uint8</span><span class="p">,</span> <span class="n">vstack</span><span class="p">,</span> <span class="n">fromstring</span><span class="p">,</span> <span class="n">sqrt</span><span class="p">,</span>\
    <span class="n">empty</span><span class="p">,</span> <span class="nb">sum</span> <span class="k">as</span> <span class="n">np_sum</span><span class="p">,</span> <span class="n">ones</span><span class="p">,</span> <span class="n">logaddexp</span>

<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="k">import</span> <span class="n">expit</span>

<span class="kn">from</span> <span class="nn">gensim</span> <span class="k">import</span> <span class="n">utils</span><span class="p">,</span> <span class="n">matutils</span>  <span class="c1"># utility fnc for pickling, common scipy operations etc</span>
<span class="kn">from</span> <span class="nn">six</span> <span class="k">import</span> <span class="n">iteritems</span><span class="p">,</span> <span class="n">itervalues</span><span class="p">,</span> <span class="n">string_types</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="k">import</span> <span class="n">GeneratorType</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec_inner</span> <span class="k">import</span> <span class="n">train_batch_sg</span><span class="p">,</span> <span class="n">train_batch_cbow</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec_inner</span> <span class="k">import</span> <span class="n">score_sentence_sg</span><span class="p">,</span> <span class="n">score_sentence_cbow</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec_inner</span> <span class="k">import</span> <span class="n">FAST_VERSION</span><span class="p">,</span> <span class="n">MAX_WORDS_IN_BATCH</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># failed... fall back to plain numpy (20-80x slower training than the above)</span>
    <span class="n">FAST_VERSION</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">MAX_WORDS_IN_BATCH</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="k">def</span> <span class="nf">train_batch_sg</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update skip-gram model by training on a sequence of sentences.</span>

<span class="sd">        Each sentence is a list of string tokens, which are looked up in the model&#39;s</span>
<span class="sd">        vocab dictionary. Called internally from `Word2Vec.train()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span> <span class="ow">and</span>
                           <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
                <span class="n">reduced_window</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>  <span class="c1"># `b` in the original word2vec code</span>

                <span class="c1"># now go over all words from the (reduced) window, predicting each one in turn</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="n">reduced_window</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:(</span><span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reduced_window</span><span class="p">)],</span> <span class="n">start</span><span class="p">):</span>
                    <span class="c1"># don&#39;t train on the `word` itself</span>
                    <span class="k">if</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">:</span>
                        <span class="n">train_sg_pair</span><span class="p">(</span>
                            <span class="n">model</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">],</span> <span class="n">word2</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="n">compute_loss</span>
                        <span class="p">)</span>

            <span class="n">result</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">train_batch_cbow</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neu1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update CBOW model by training on a sequence of sentences.</span>

<span class="sd">        Each sentence is a list of string tokens, which are looked up in the model&#39;s</span>
<span class="sd">        vocab dictionary. Called internally from `Word2Vec.train()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span> <span class="ow">and</span>
                           <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
                <span class="n">reduced_window</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>  <span class="c1"># `b` in the original word2vec code</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="n">reduced_window</span><span class="p">)</span>
                <span class="n">window_pos</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:(</span><span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reduced_window</span><span class="p">)],</span> <span class="n">start</span><span class="p">)</span>
                <span class="n">word2_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2</span><span class="o">.</span><span class="n">index</span> <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="n">window_pos</span> <span class="k">if</span> <span class="p">(</span><span class="n">word2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">)]</span>
                <span class="n">l1</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2_indices</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 1 x vector_size</span>
                <span class="k">if</span> <span class="n">word2_indices</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span><span class="p">:</span>
                    <span class="n">l1</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2_indices</span><span class="p">)</span>
                <span class="n">train_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2_indices</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="n">compute_loss</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">score_sentence_sg</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Obtain likelihood score for a single sentence in a fitted skip-gram representaion.</span>

<span class="sd">        The sentence is a list of Vocab objects (or None, when the corresponding</span>
<span class="sd">        word is not in the vocabulary). Called internally from `Word2Vec.score()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">log_prob_sentence</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;scoring is only available for HS=True&quot;</span><span class="p">)</span>

        <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># OOV word in the input sentence =&gt; skip</span>

            <span class="c1"># now go over all words from the window, predicting each one in turn</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">start</span><span class="p">):</span>
                <span class="c1"># don&#39;t train on OOV words and on the `word` itself</span>
                <span class="k">if</span> <span class="n">word2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">:</span>
                    <span class="n">log_prob_sentence</span> <span class="o">+=</span> <span class="n">score_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_sentence</span>

    <span class="k">def</span> <span class="nf">score_sentence_cbow</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neu1</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Obtain likelihood score for a single sentence in a fitted CBOW representaion.</span>

<span class="sd">        The sentence is a list of Vocab objects (or None, where the corresponding</span>
<span class="sd">        word is not in the vocabulary. Called internally from `Word2Vec.score()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">log_prob_sentence</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;scoring is only available for HS=True&quot;</span><span class="p">)</span>

        <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># OOV word in the input sentence =&gt; skip</span>

            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>
            <span class="n">window_pos</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:(</span><span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">start</span><span class="p">)</span>
            <span class="n">word2_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2</span><span class="o">.</span><span class="n">index</span> <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="n">window_pos</span> <span class="k">if</span> <span class="p">(</span><span class="n">word2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">)]</span>
            <span class="n">l1</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2_indices</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 1 x layer1_size</span>
            <span class="k">if</span> <span class="n">word2_indices</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span><span class="p">:</span>
                <span class="n">l1</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2_indices</span><span class="p">)</span>
            <span class="n">log_prob_sentence</span> <span class="o">+=</span> <span class="n">score_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_sentence</span>


<span class="k">def</span> <span class="nf">train_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">context_index</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">learn_vectors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">learn_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">context_vectors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">context_locks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_ft</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context_vectors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
            <span class="n">context_vectors_vocab</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0_vocab</span>
            <span class="n">context_vectors_ngrams</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0_ngrams</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span>
    <span class="k">if</span> <span class="n">context_locks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
            <span class="n">context_locks_vocab</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_vocab_lockf</span>
            <span class="n">context_locks_ngrams</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_ngrams_lockf</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_locks</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_lockf</span>

    <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">predict_word</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>  <span class="c1"># target word (NN output)</span>

    <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
        <span class="n">l1_vocab</span> <span class="o">=</span> <span class="n">context_vectors_vocab</span><span class="p">[</span><span class="n">context_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">l1_ngrams</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">(</span><span class="n">context_vectors_ngrams</span><span class="p">[</span><span class="n">context_index</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context_index</span><span class="p">:</span>
            <span class="n">l1</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">([</span><span class="n">l1_vocab</span><span class="p">,</span> <span class="n">l1_ngrams</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">context_index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">l1</span> <span class="o">=</span> <span class="n">context_vectors</span><span class="p">[</span><span class="n">context_index</span><span class="p">]</span>  <span class="c1"># input word (NN input/projection layer)</span>
        <span class="n">lock_factor</span> <span class="o">=</span> <span class="n">context_locks</span><span class="p">[</span><span class="n">context_index</span><span class="p">]</span>

    <span class="n">neu1e</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">l1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
        <span class="c1"># work on the entire tree at once, to push as much work into numpy&#39;s C routines as possible (performance)</span>
        <span class="n">l2a</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">predict_word</span><span class="o">.</span><span class="n">point</span><span class="p">])</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
        <span class="n">prod_term</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">fa</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">prod_term</span><span class="p">)</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">ga</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predict_word</span><span class="o">.</span><span class="n">code</span> <span class="o">-</span> <span class="n">fa</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">predict_word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l2a</span><span class="p">)</span>  <span class="c1"># save error</span>

        <span class="c1"># loss component corresponding to hierarchical softmax</span>
        <span class="k">if</span> <span class="n">compute_loss</span><span class="p">:</span>
            <span class="n">sgn</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="n">predict_word</span><span class="o">.</span><span class="n">code</span>  <span class="c1"># `ch` function, 0 -&gt; 1, 1 -&gt; -1</span>
            <span class="n">lprob</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="o">-</span><span class="n">sgn</span> <span class="o">*</span> <span class="n">prod_term</span><span class="p">))</span>
            <span class="n">model</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lprob</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
        <span class="c1"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span>
        <span class="n">word_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">predict_word</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">predict_word</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
                <span class="n">word_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">l2b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span>  <span class="c1"># 2d matrix, k+1 x layer1_size</span>
        <span class="n">prod_term</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2b</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">fb</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">prod_term</span><span class="p">)</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">-</span> <span class="n">fb</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l2b</span><span class="p">)</span>  <span class="c1"># save error</span>

        <span class="c1"># loss component corresponding to negative sampling</span>
        <span class="k">if</span> <span class="n">compute_loss</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">-=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">prod_term</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>  <span class="c1"># for the sampled words</span>
            <span class="n">model</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">-=</span> <span class="n">log</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="n">prod_term</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># for the output word</span>

    <span class="k">if</span> <span class="n">learn_vectors</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0_vocab</span><span class="p">[</span><span class="n">context_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">context_locks_vocab</span><span class="p">[</span><span class="n">context_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">context_index</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0_ngrams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">context_locks_ngrams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">l1</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">lock_factor</span>  <span class="c1"># learn input -&gt; hidden (mutates model.wv.syn0[word2.index], if that is l1)</span>
    <span class="k">return</span> <span class="n">neu1e</span>


<span class="k">def</span> <span class="nf">train_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">input_word_indices</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">learn_vectors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">learn_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">context_vectors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">context_locks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_ft</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context_vectors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
            <span class="n">context_vectors_vocab</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0_vocab</span>
            <span class="n">context_vectors_ngrams</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0_ngrams</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span>
    <span class="k">if</span> <span class="n">context_locks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
            <span class="n">context_locks_vocab</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_vocab_lockf</span>
            <span class="n">context_locks_ngrams</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_ngrams_lockf</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context_locks</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_lockf</span>

    <span class="n">neu1e</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">l1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
        <span class="n">l2a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
        <span class="n">prod_term</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">fa</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">prod_term</span><span class="p">)</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">ga</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">word</span><span class="o">.</span><span class="n">code</span> <span class="o">-</span> <span class="n">fa</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l2a</span><span class="p">)</span>  <span class="c1"># save error</span>

        <span class="c1"># loss component corresponding to hierarchical softmax</span>
        <span class="k">if</span> <span class="n">compute_loss</span><span class="p">:</span>
            <span class="n">sgn</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="n">word</span><span class="o">.</span><span class="n">code</span>  <span class="c1"># ch function, 0-&gt; 1, 1 -&gt; -1</span>
            <span class="n">model</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="o">-</span><span class="n">sgn</span> <span class="o">*</span> <span class="n">prod_term</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
        <span class="c1"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span>
        <span class="n">word_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
                <span class="n">word_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">l2b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span>  <span class="c1"># 2d matrix, k+1 x layer1_size</span>
        <span class="n">prod_term</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2b</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">fb</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">prod_term</span><span class="p">)</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">-</span> <span class="n">fb</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l2b</span><span class="p">)</span>  <span class="c1"># save error</span>

        <span class="c1"># loss component corresponding to negative sampling</span>
        <span class="k">if</span> <span class="n">compute_loss</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">-=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">prod_term</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>  <span class="c1"># for the sampled words</span>
            <span class="n">model</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">-=</span> <span class="n">log</span><span class="p">(</span><span class="n">expit</span><span class="p">(</span><span class="n">prod_term</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># for the output word</span>

    <span class="k">if</span> <span class="n">learn_vectors</span><span class="p">:</span>
        <span class="c1"># learn input -&gt; hidden, here for all words in the window separately</span>
        <span class="k">if</span> <span class="n">is_ft</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span> <span class="ow">and</span> <span class="n">input_word_indices</span><span class="p">:</span>
                <span class="n">neu1e</span> <span class="o">/=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_word_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_word_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">input_word_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">context_vectors_vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">context_locks_vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">input_word_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">context_vectors_ngrams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">context_locks_ngrams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span> <span class="ow">and</span> <span class="n">input_word_indices</span><span class="p">:</span>
                <span class="n">neu1e</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_word_indices</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">input_word_indices</span><span class="p">:</span>
                <span class="n">context_vectors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">context_locks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">neu1e</span>


<span class="k">def</span> <span class="nf">score_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2</span><span class="p">):</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
    <span class="n">l2a</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">])</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
    <span class="n">sgn</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="n">word</span><span class="o">.</span><span class="n">code</span>  <span class="c1"># ch function, 0-&gt; 1, 1 -&gt; -1</span>
    <span class="n">lprob</span> <span class="o">=</span> <span class="o">-</span><span class="n">logaddexp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">sgn</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lprob</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">score_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">l1</span><span class="p">):</span>
    <span class="n">l2a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
    <span class="n">sgn</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="n">word</span><span class="o">.</span><span class="n">code</span>  <span class="c1"># ch function, 0-&gt; 1, 1 -&gt; -1</span>
    <span class="n">lprob</span> <span class="o">=</span> <span class="o">-</span><span class="n">logaddexp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">sgn</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lprob</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">SaveLoad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for training, using and evaluating neural networks described in https://code.google.com/p/word2vec/</span>

<span class="sd">    If you&#39;re finished training a model (=no more updates, only querying)</span>
<span class="sd">    then switch to the :mod:`gensim.models.KeyedVectors` instance in wv</span>

<span class="sd">    The model can be stored/loaded via its `save()` and `load()` methods, or stored/loaded in a format</span>
<span class="sd">    compatible with the original word2vec implementation via `wv.save_word2vec_format()` and `KeyedVectors.load_word2vec_format()`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">max_vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hashfxn</span><span class="o">=</span><span class="nb">hash</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">null_word</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">trim_rule</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sorted_vocab</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_words</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the model from an iterable of `sentences`. Each sentence is a</span>
<span class="sd">        list of words (unicode strings) that will be used for training.</span>

<span class="sd">        The `sentences` iterable can be simply a list, but for larger corpora,</span>
<span class="sd">        consider an iterable that streams the sentences directly from disk/network.</span>
<span class="sd">        See :class:`BrownCorpus`, :class:`Text8Corpus` or :class:`LineSentence` in</span>
<span class="sd">        this module for such examples.</span>

<span class="sd">        If you don&#39;t supply `sentences`, the model is left uninitialized -- use if</span>
<span class="sd">        you plan to initialize it in some other way.</span>

<span class="sd">        `sg` defines the training algorithm. By default (`sg=0`), CBOW is used.</span>
<span class="sd">        Otherwise (`sg=1`), skip-gram is employed.</span>

<span class="sd">        `size` is the dimensionality of the feature vectors.</span>

<span class="sd">        `window` is the maximum distance between the current and predicted word within a sentence.</span>

<span class="sd">        `alpha` is the initial learning rate (will linearly drop to `min_alpha` as training progresses).</span>

<span class="sd">        `seed` = for the random number generator. Initial vectors for each</span>
<span class="sd">        word are seeded with a hash of the concatenation of word + str(seed).</span>
<span class="sd">        Note that for a fully deterministically-reproducible run, you must also limit the model to</span>
<span class="sd">        a single worker thread, to eliminate ordering jitter from OS thread scheduling. (In Python</span>
<span class="sd">        3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED</span>
<span class="sd">        environment variable to control hash randomization.)</span>

<span class="sd">        `min_count` = ignore all words with total frequency lower than this.</span>

<span class="sd">        `max_vocab_size` = limit RAM during vocabulary building; if there are more unique</span>
<span class="sd">        words than this, then prune the infrequent ones. Every 10 million word types</span>
<span class="sd">        need about 1GB of RAM. Set to `None` for no limit (default).</span>

<span class="sd">        `sample` = threshold for configuring which higher-frequency words are randomly downsampled;</span>
<span class="sd">            default is 1e-3, useful range is (0, 1e-5).</span>

<span class="sd">        `workers` = use this many worker threads to train the model (=faster training with multicore machines).</span>

<span class="sd">        `hs` = if 1, hierarchical softmax will be used for model training.</span>
<span class="sd">        If set to 0 (default), and `negative` is non-zero, negative sampling will be used.</span>

<span class="sd">        `negative` = if &gt; 0, negative sampling will be used, the int for negative</span>
<span class="sd">        specifies how many &quot;noise words&quot; should be drawn (usually between 5-20).</span>
<span class="sd">        Default is 5. If set to 0, no negative samping is used.</span>

<span class="sd">        `cbow_mean` = if 0, use the sum of the context word vectors. If 1 (default), use the mean.</span>
<span class="sd">        Only applies when cbow is used.</span>

<span class="sd">        `hashfxn` = hash function to use to randomly initialize weights, for increased</span>
<span class="sd">        training reproducibility. Default is Python&#39;s rudimentary built in hash function.</span>

<span class="sd">        `iter` = number of iterations (epochs) over the corpus. Default is 5.</span>

<span class="sd">        `trim_rule` = vocabulary trimming rule, specifies whether certain words should remain</span>
<span class="sd">        in the vocabulary, be trimmed away, or handled using the default (discard if word count &lt; min_count).</span>
<span class="sd">        Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and</span>
<span class="sd">        returns either `utils.RULE_DISCARD`, `utils.RULE_KEEP` or `utils.RULE_DEFAULT`.</span>
<span class="sd">        Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part</span>
<span class="sd">        of the model.</span>

<span class="sd">        `sorted_vocab` = if 1 (default), sort the vocabulary by descending frequency before</span>
<span class="sd">        assigning word indexes.</span>

<span class="sd">        `batch_words` = target size (in words) for batches of examples passed to worker threads (and</span>
<span class="sd">        thus cython routines). Default is 10000. (Larger batches will be passed if individual</span>
<span class="sd">        texts are longer than 10000 words, but the standard cython code truncates to that maximum.)</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load</span> <span class="o">=</span> <span class="n">call_on_class_only</span>

        <span class="k">if</span> <span class="n">FAST_VERSION</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Slow version of </span><span class="si">%s</span><span class="s1"> is being used&#39;</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Fast version of </span><span class="si">%s</span><span class="s1"> is being used&#39;</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_word_vectors</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># for negative sampling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;consider setting layer size to a multiple of 4 for greater performance&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha_yet_reached</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>  <span class="c1"># To warn user if alpha increases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span> <span class="o">=</span> <span class="n">max_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">workers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">min_alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hs</span> <span class="o">=</span> <span class="n">hs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">=</span> <span class="n">negative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cbow_mean</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cbow_mean</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hashfxn</span> <span class="o">=</span> <span class="n">hashfxn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">=</span> <span class="nb">iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">null_word</span> <span class="o">=</span> <span class="n">null_word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_train_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sorted_vocab</span> <span class="o">=</span> <span class="n">sorted_vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_words</span> <span class="o">=</span> <span class="n">batch_words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_trimmed_post_training</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span> <span class="o">=</span> <span class="n">compute_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">sentences</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">GeneratorType</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;You can&#39;t pass a generator as the sentences argument. Try an iterator.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">start_alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">end_alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">trim_rule</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The rule, if given, is only used to prune vocabulary during build_vocab() &quot;</span>
                    <span class="s2">&quot;and is not stored as part of the model. Model initialized without sentences. &quot;</span>
                    <span class="s2">&quot;trim_rule provided, if any, will be ignored.&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">initialize_word_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">make_cum_table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a cumulative-distribution table using stored vocabulary word counts for</span>
<span class="sd">        drawing random words in the negative-sampling training routines.</span>

<span class="sd">        To draw a word index, choose a random integer up to the maximum value in the</span>
<span class="sd">        table (cum_table[-1]), then finding that integer&#39;s sorted insertion point</span>
<span class="sd">        (as if by bisect_left or ndarray.searchsorted()). That insertion point is the</span>
<span class="sd">        drawn index, coming up in proportion equal to the increment at that slot.</span>

<span class="sd">        Called internally from &#39;build_vocab()&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint32</span><span class="p">)</span>
        <span class="c1"># compute sum of all power (Z in paper)</span>
        <span class="n">train_words_pow</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">word_index</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
            <span class="n">train_words_pow</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">word_index</span><span class="p">]]</span><span class="o">.</span><span class="n">count</span><span class="o">**</span><span class="n">power</span>
        <span class="n">cumulative</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">word_index</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
            <span class="n">cumulative</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">word_index</span><span class="p">]]</span><span class="o">.</span><span class="n">count</span><span class="o">**</span><span class="n">power</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">cumulative</span> <span class="o">/</span> <span class="n">train_words_pow</span> <span class="o">*</span> <span class="n">domain</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">domain</span>

    <span class="k">def</span> <span class="nf">create_binary_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a binary Huffman tree using stored vocabulary word counts. Frequent words</span>
<span class="sd">        will have shorter binary codes. Called internally from `build_vocab()`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;constructing a huffman tree from </span><span class="si">%i</span><span class="s2"> words&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>

        <span class="c1"># build the huffman tree</span>
        <span class="n">heap</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
        <span class="n">heapq</span><span class="o">.</span><span class="n">heapify</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">min1</span><span class="p">,</span> <span class="n">min2</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">),</span> <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
            <span class="n">heapq</span><span class="o">.</span><span class="n">heappush</span><span class="p">(</span>
                <span class="n">heap</span><span class="p">,</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="n">min1</span><span class="o">.</span><span class="n">count</span> <span class="o">+</span> <span class="n">min2</span><span class="o">.</span><span class="n">count</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">left</span><span class="o">=</span><span class="n">min1</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="n">min2</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># recurse over the tree, assigning a binary code to each vocabulary word</span>
        <span class="k">if</span> <span class="n">heap</span><span class="p">:</span>
            <span class="n">max_depth</span><span class="p">,</span> <span class="n">stack</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[(</span><span class="n">heap</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[])]</span>
            <span class="k">while</span> <span class="n">stack</span><span class="p">:</span>
                <span class="n">node</span><span class="p">,</span> <span class="n">codes</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">):</span>
                    <span class="c1"># leaf node =&gt; store its path from the root</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">point</span> <span class="o">=</span> <span class="n">codes</span><span class="p">,</span> <span class="n">points</span>
                    <span class="n">max_depth</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">codes</span><span class="p">),</span> <span class="n">max_depth</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># inner node =&gt; continue recursion</span>
                    <span class="n">points</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">index</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint32</span><span class="p">)</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">),</span> <span class="n">points</span><span class="p">))</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="p">,</span> <span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">),</span> <span class="n">points</span><span class="p">))</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;built huffman tree with maximum node depth </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">keep_raw_vocab</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">progress_per</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build vocabulary from a sequence of sentences (can be a once-only generator stream).</span>
<span class="sd">        Each sentence must be a list of unicode strings.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scan_vocab</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">progress_per</span><span class="o">=</span><span class="n">progress_per</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>  <span class="c1"># initial survey</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_vocab</span><span class="p">(</span><span class="n">keep_raw_vocab</span><span class="o">=</span><span class="n">keep_raw_vocab</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="n">update</span><span class="p">)</span>  <span class="c1"># trim by min_count &amp; precalculate downsampling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">finalize_vocab</span><span class="p">(</span><span class="n">update</span><span class="o">=</span><span class="n">update</span><span class="p">)</span>  <span class="c1"># build tables &amp; arrays</span>

    <span class="k">def</span> <span class="nf">scan_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">progress_per</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do an initial scan of all words appearing in sentences.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;collecting all words and their counts&quot;</span><span class="p">)</span>
        <span class="n">sentence_no</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">total_words</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">min_reduce</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">checked_string_types</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">sentence_no</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">checked_string_types</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">string_types</span><span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Each &#39;sentences&#39; item should be a list of words (usually unicode strings). &quot;</span>
                        <span class="s2">&quot;First item here is instead plain </span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span>
                        <span class="nb">type</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">checked_string_types</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">sentence_no</span> <span class="o">%</span> <span class="n">progress_per</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;PROGRESS: at sentence #</span><span class="si">%i</span><span class="s2">, processed </span><span class="si">%i</span><span class="s2"> words, keeping </span><span class="si">%i</span><span class="s2"> word types&quot;</span><span class="p">,</span>
                    <span class="n">sentence_no</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">itervalues</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span> <span class="o">+</span> <span class="n">total_words</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
                <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span><span class="p">:</span>
                <span class="n">total_words</span> <span class="o">+=</span> <span class="n">utils</span><span class="o">.</span><span class="n">prune_vocab</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">min_reduce</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>
                <span class="n">min_reduce</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">total_words</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">itervalues</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;collected </span><span class="si">%i</span><span class="s2"> word types from a corpus of </span><span class="si">%i</span><span class="s2"> raw words and </span><span class="si">%i</span><span class="s2"> sentences&quot;</span><span class="p">,</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">total_words</span><span class="p">,</span> <span class="n">sentence_no</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span> <span class="o">=</span> <span class="n">sentence_no</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span> <span class="o">=</span> <span class="n">vocab</span>

    <span class="k">def</span> <span class="nf">scale_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dry_run</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">keep_raw_vocab</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply vocabulary settings for `min_count` (discarding less-frequent words)</span>
<span class="sd">        and `sample` (controlling the downsampling of more-frequent words).</span>

<span class="sd">        Calling with `dry_run=True` will only simulate the provided settings and</span>
<span class="sd">        report the size of the retained vocabulary, effective corpus length, and</span>
<span class="sd">        estimated memory requirements. Results are both printed via logging and</span>
<span class="sd">        returned as a dict.</span>

<span class="sd">        Delete the raw vocabulary after the scaling is done to free up RAM,</span>
<span class="sd">        unless `keep_raw_vocab` is set.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span>
        <span class="n">drop_total</span> <span class="o">=</span> <span class="n">drop_unique</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">update</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading a fresh vocabulary&quot;</span><span class="p">)</span>
            <span class="n">retain_total</span><span class="p">,</span> <span class="n">retain_words</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[]</span>
            <span class="c1"># Discard words less-frequent than min_count</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="c1"># make stored settings match these applied settings</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">keep_vocab_item</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">):</span>
                    <span class="n">retain_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
                    <span class="n">retain_total</span> <span class="o">+=</span> <span class="n">v</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">))</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">drop_unique</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">drop_total</span> <span class="o">+=</span> <span class="n">v</span>
            <span class="n">original_unique_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">retain_words</span><span class="p">)</span> <span class="o">+</span> <span class="n">drop_unique</span>
            <span class="n">retain_unique_pct</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">retain_words</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">original_unique_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;min_count=</span><span class="si">%d</span><span class="s2"> retains </span><span class="si">%i</span><span class="s2"> unique words (</span><span class="si">%i%%</span><span class="s2"> of original </span><span class="si">%i</span><span class="s2">, drops </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
                <span class="n">min_count</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">retain_words</span><span class="p">),</span> <span class="n">retain_unique_pct</span><span class="p">,</span> <span class="n">original_unique_total</span><span class="p">,</span> <span class="n">drop_unique</span>
            <span class="p">)</span>
            <span class="n">original_total</span> <span class="o">=</span> <span class="n">retain_total</span> <span class="o">+</span> <span class="n">drop_total</span>
            <span class="n">retain_pct</span> <span class="o">=</span> <span class="n">retain_total</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">original_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;min_count=</span><span class="si">%d</span><span class="s2"> leaves </span><span class="si">%i</span><span class="s2"> word corpus (</span><span class="si">%i%%</span><span class="s2"> of original </span><span class="si">%i</span><span class="s2">, drops </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
                <span class="n">min_count</span><span class="p">,</span> <span class="n">retain_total</span><span class="p">,</span> <span class="n">retain_pct</span><span class="p">,</span> <span class="n">original_total</span><span class="p">,</span> <span class="n">drop_total</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Updating model with new vocabulary&quot;</span><span class="p">)</span>
            <span class="n">new_total</span> <span class="o">=</span> <span class="n">pre_exist_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">new_words</span> <span class="o">=</span> <span class="n">pre_exist_words</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">keep_vocab_item</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                        <span class="n">pre_exist_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
                        <span class="n">pre_exist_total</span> <span class="o">+=</span> <span class="n">v</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">v</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
                        <span class="n">new_total</span> <span class="o">+=</span> <span class="n">v</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">))</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">drop_unique</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">drop_total</span> <span class="o">+=</span> <span class="n">v</span>
            <span class="n">original_unique_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pre_exist_words</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_words</span><span class="p">)</span> <span class="o">+</span> <span class="n">drop_unique</span>
            <span class="n">pre_exist_unique_pct</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pre_exist_words</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">original_unique_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">new_unique_pct</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_words</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">original_unique_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;New added </span><span class="si">%i</span><span class="s2"> unique words (</span><span class="si">%i%%</span><span class="s2"> of original </span><span class="si">%i</span><span class="s2">) &quot;</span>
                <span class="s2">&quot;and increased the count of </span><span class="si">%i</span><span class="s2"> pre-existing words (</span><span class="si">%i%%</span><span class="s2"> of original </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">new_words</span><span class="p">),</span> <span class="n">new_unique_pct</span><span class="p">,</span> <span class="n">original_unique_total</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pre_exist_words</span><span class="p">),</span>
                <span class="n">pre_exist_unique_pct</span><span class="p">,</span> <span class="n">original_unique_total</span>
            <span class="p">)</span>
            <span class="n">retain_words</span> <span class="o">=</span> <span class="n">new_words</span> <span class="o">+</span> <span class="n">pre_exist_words</span>
            <span class="n">retain_total</span> <span class="o">=</span> <span class="n">new_total</span> <span class="o">+</span> <span class="n">pre_exist_total</span>

        <span class="c1"># Precalculate each vocabulary item&#39;s threshold for sampling</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sample</span><span class="p">:</span>
            <span class="c1"># no words downsampled</span>
            <span class="n">threshold_count</span> <span class="o">=</span> <span class="n">retain_total</span>
        <span class="k">elif</span> <span class="n">sample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="c1"># traditional meaning: set parameter as proportion of total</span>
            <span class="n">threshold_count</span> <span class="o">=</span> <span class="n">sample</span> <span class="o">*</span> <span class="n">retain_total</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># new shorthand: sample &gt;= 1 means downsample all words with higher count than sample</span>
            <span class="n">threshold_count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">downsample_total</span><span class="p">,</span> <span class="n">downsample_unique</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">retain_words</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="n">word_probability</span> <span class="o">=</span> <span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">/</span> <span class="n">threshold_count</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">threshold_count</span> <span class="o">/</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">word_probability</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">downsample_unique</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">downsample_total</span> <span class="o">+=</span> <span class="n">word_probability</span> <span class="o">*</span> <span class="n">v</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_probability</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="n">downsample_total</span> <span class="o">+=</span> <span class="n">v</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">word_probability</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">keep_raw_vocab</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;deleting the raw counts dictionary of </span><span class="si">%i</span><span class="s2"> items&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;sample=</span><span class="si">%g</span><span class="s2"> downsamples </span><span class="si">%i</span><span class="s2"> most-common words&quot;</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">downsample_unique</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;downsampling leaves estimated </span><span class="si">%i</span><span class="s2"> word corpus (</span><span class="si">%.1f%%</span><span class="s2"> of prior </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
            <span class="n">downsample_total</span><span class="p">,</span> <span class="n">downsample_total</span> <span class="o">*</span> <span class="mf">100.0</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">retain_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">retain_total</span>
        <span class="p">)</span>

        <span class="c1"># return from each step: words-affected, resulting-corpus-size, extra memory estimates</span>
        <span class="n">report_values</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;drop_unique&#39;</span><span class="p">:</span> <span class="n">drop_unique</span><span class="p">,</span> <span class="s1">&#39;retain_total&#39;</span><span class="p">:</span> <span class="n">retain_total</span><span class="p">,</span> <span class="s1">&#39;downsample_unique&#39;</span><span class="p">:</span> <span class="n">downsample_unique</span><span class="p">,</span>
            <span class="s1">&#39;downsample_total&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">downsample_total</span><span class="p">),</span> <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_memory</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">retain_words</span><span class="p">))</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">report_values</span>

    <span class="k">def</span> <span class="nf">finalize_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build tables and model weights based on final vocabulary settings.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_vocab</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sorted_vocab</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">update</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sort_vocab</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="c1"># add info about each word&#39;s Huffman encoding</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_binary_tree</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="c1"># build the table for drawing random words (for negative sampling)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_cum_table</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">null_word</span><span class="p">:</span>
            <span class="c1"># create null pseudo-word for padding when using concatenative L1 (run-of-words)</span>
            <span class="c1"># this word is only ever input – never predicted – so count, huffman-point, etc doesn&#39;t matter</span>
            <span class="n">word</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\0</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">v</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># set initial input/projection and hidden weights</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">update</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_weights</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">sort_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sort the vocabulary so the most frequent words have the lowest indexes.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;cannot sort vocabulary after model weights already initialized.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">i</span>

    <span class="k">def</span> <span class="nf">reset_from</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other_model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Borrow shareable pre-built structures (like vocab) from the other_model. Useful</span>
<span class="sd">        if testing multiple models in parallel on the same corpus.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">cum_table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">corpus_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_do_train_job</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">inits</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train a single batch of sentences. Return 2-tuple `(effective word count after</span>
<span class="sd">        ignoring unknown words and sentence length trimming, total word count)`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">work</span><span class="p">,</span> <span class="n">neu1</span> <span class="o">=</span> <span class="n">inits</span>
        <span class="n">tally</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">:</span>
            <span class="n">tally</span> <span class="o">+=</span> <span class="n">train_batch_sg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tally</span> <span class="o">+=</span> <span class="n">train_batch_cbow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="p">,</span> <span class="n">neu1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tally</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_word_count</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_raw_word_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">job</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the number of words in a given job.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">job</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">total_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end_alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">word_count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">queue_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">report_delay</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update the model&#39;s neural weights from a sequence of sentences (can be a once-only generator stream).</span>
<span class="sd">        For Word2Vec, each sentence must be a list of unicode strings. (Subclasses may accept other examples.)</span>

<span class="sd">        To support linear learning-rate decay from (initial) alpha to min_alpha, and accurate</span>
<span class="sd">        progres-percentage logging, either total_examples (count of sentences) or total_words (count of</span>
<span class="sd">        raw words in sentences) MUST be provided. (If the corpus is the same as was provided to</span>
<span class="sd">        `build_vocab()`, the count of examples in that corpus will be available in the model&#39;s</span>
<span class="sd">        `corpus_count` property.)</span>

<span class="sd">        To avoid common mistakes around the model&#39;s ability to do multiple training passes itself, an</span>
<span class="sd">        explicit `epochs` argument MUST be provided. In the common and recommended case, where `train()`</span>
<span class="sd">        is only called once, the model&#39;s cached `iter` value should be supplied as `epochs` value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_trimmed_post_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Parameters for training were discarded using model_trimmed_post_training method&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">FAST_VERSION</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;C extension not loaded for Word2Vec, training will be slow. &quot;</span>
                <span class="s2">&quot;Install a C compiler and reinstall gensim for fast training.&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># precompute negative labels optimization for pure-python training</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">neg_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>

        <span class="k">if</span> <span class="n">compute_loss</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span> <span class="o">=</span> <span class="n">compute_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_training_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;training model with </span><span class="si">%i</span><span class="s2"> workers on </span><span class="si">%i</span><span class="s2"> vocabulary and </span><span class="si">%i</span><span class="s2"> features, &quot;</span>
            <span class="s2">&quot;using sg=</span><span class="si">%s</span><span class="s2"> hs=</span><span class="si">%s</span><span class="s2"> sample=</span><span class="si">%s</span><span class="s2"> negative=</span><span class="si">%s</span><span class="s2"> window=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;you must first build vocabulary before training the model&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;you must first finalize vocabulary before training the model&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;corpus_count&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The number of sentences in the training corpus is missing. Did you load the model via KeyedVectors.load_word2vec_format?&quot;</span>
                <span class="s2">&quot;Models loaded via load_word2vec_format don&#39;t support further training. &quot;</span>
                <span class="s2">&quot;Instead start with a blank model, scan_vocab on the new corpus, &quot;</span>
                <span class="s2">&quot;intersect_word2vec_format with the old model, then train.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">total_words</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">total_examples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must specify either total_examples or total_words, for proper alpha and progress calculations. &quot;</span>
                <span class="s2">&quot;The usual value is total_examples=model.corpus_count.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">epochs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must specify an explict epochs count. The usual value is epochs=model.iter.&quot;</span><span class="p">)</span>
        <span class="n">start_alpha</span> <span class="o">=</span> <span class="n">start_alpha</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">end_alpha</span> <span class="o">=</span> <span class="n">end_alpha</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span>

        <span class="n">job_tally</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">epochs</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">RepeatCorpusNTimes</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
            <span class="n">total_words</span> <span class="o">=</span> <span class="n">total_words</span> <span class="ow">and</span> <span class="n">total_words</span> <span class="o">*</span> <span class="n">epochs</span>
            <span class="n">total_examples</span> <span class="o">=</span> <span class="n">total_examples</span> <span class="ow">and</span> <span class="n">total_examples</span> <span class="o">*</span> <span class="n">epochs</span>

        <span class="k">def</span> <span class="nf">worker_loop</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Train the model, lifting lists of sentences from the job_queue.&quot;&quot;&quot;</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># per-thread private work memory</span>
            <span class="n">neu1</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
            <span class="n">jobs_processed</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">job</span> <span class="o">=</span> <span class="n">job_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">job</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">progress_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                    <span class="k">break</span>  <span class="c1"># no more jobs =&gt; quit this worker</span>
                <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">job</span>
                <span class="n">tally</span><span class="p">,</span> <span class="n">raw_tally</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_train_job</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="n">work</span><span class="p">,</span> <span class="n">neu1</span><span class="p">))</span>
                <span class="n">progress_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tally</span><span class="p">,</span> <span class="n">raw_tally</span><span class="p">))</span>  <span class="c1"># report back progress</span>
                <span class="n">jobs_processed</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;worker exiting, processed </span><span class="si">%i</span><span class="s2"> jobs&quot;</span><span class="p">,</span> <span class="n">jobs_processed</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">job_producer</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Fill jobs queue using the input `sentences` iterator.&quot;&quot;&quot;</span>
            <span class="n">job_batch</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="p">[],</span> <span class="mi">0</span>
            <span class="n">pushed_words</span><span class="p">,</span> <span class="n">pushed_examples</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
            <span class="n">next_alpha</span> <span class="o">=</span> <span class="n">start_alpha</span>
            <span class="k">if</span> <span class="n">next_alpha</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha_yet_reached</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Effective &#39;alpha&#39; higher than previous training cycles&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha_yet_reached</span> <span class="o">=</span> <span class="n">next_alpha</span>
            <span class="n">job_no</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">for</span> <span class="n">sent_idx</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
                <span class="n">sentence_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_word_count</span><span class="p">([</span><span class="n">sentence</span><span class="p">])</span>

                <span class="c1"># can we fit this sentence into the existing job batch?</span>
                <span class="k">if</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="n">sentence_length</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_words</span><span class="p">:</span>
                    <span class="c1"># yes =&gt; add it to the current job</span>
                    <span class="n">job_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
                    <span class="n">batch_size</span> <span class="o">+=</span> <span class="n">sentence_length</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># no =&gt; submit the existing job</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                        <span class="s2">&quot;queueing job #</span><span class="si">%i</span><span class="s2"> (</span><span class="si">%i</span><span class="s2"> words, </span><span class="si">%i</span><span class="s2"> sentences) at alpha </span><span class="si">%.05f</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">job_no</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">job_batch</span><span class="p">),</span> <span class="n">next_alpha</span>
                    <span class="p">)</span>
                    <span class="n">job_no</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">job_batch</span><span class="p">,</span> <span class="n">next_alpha</span><span class="p">))</span>

                    <span class="c1"># update the learning rate for the next job</span>
                    <span class="k">if</span> <span class="n">end_alpha</span> <span class="o">&lt;</span> <span class="n">next_alpha</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">total_examples</span><span class="p">:</span>
                            <span class="c1"># examples-based decay</span>
                            <span class="n">pushed_examples</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">job_batch</span><span class="p">)</span>
                            <span class="n">progress</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">pushed_examples</span> <span class="o">/</span> <span class="n">total_examples</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># words-based decay</span>
                            <span class="n">pushed_words</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_word_count</span><span class="p">(</span><span class="n">job_batch</span><span class="p">)</span>
                            <span class="n">progress</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">pushed_words</span> <span class="o">/</span> <span class="n">total_words</span>
                        <span class="n">next_alpha</span> <span class="o">=</span> <span class="n">start_alpha</span> <span class="o">-</span> <span class="p">(</span><span class="n">start_alpha</span> <span class="o">-</span> <span class="n">end_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">progress</span>
                        <span class="n">next_alpha</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">end_alpha</span><span class="p">,</span> <span class="n">next_alpha</span><span class="p">)</span>

                    <span class="c1"># add the sentence that didn&#39;t fit as the first item of a new job</span>
                    <span class="n">job_batch</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="p">],</span> <span class="n">sentence_length</span>

            <span class="c1"># add the last job too (may be significantly smaller than batch_words)</span>
            <span class="k">if</span> <span class="n">job_batch</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="s2">&quot;queueing job #</span><span class="si">%i</span><span class="s2"> (</span><span class="si">%i</span><span class="s2"> words, </span><span class="si">%i</span><span class="s2"> sentences) at alpha </span><span class="si">%.05f</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">job_no</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">job_batch</span><span class="p">),</span> <span class="n">next_alpha</span>
                <span class="p">)</span>
                <span class="n">job_no</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">job_batch</span><span class="p">,</span> <span class="n">next_alpha</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">job_no</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;train() called with an empty iterator (if not intended, &quot;</span>
                    <span class="s2">&quot;be sure to provide a corpus that offers restartable iteration = an iterable).&quot;</span>
                <span class="p">)</span>

            <span class="c1"># give the workers heads up that they can finish -- no more work!</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">):</span>
                <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;job loop exiting, total </span><span class="si">%i</span><span class="s2"> jobs&quot;</span><span class="p">,</span> <span class="n">job_no</span><span class="p">)</span>

        <span class="c1"># buffer ahead only a limited number of jobs.. this is the reason we can&#39;t simply use ThreadPool :(</span>
        <span class="n">job_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="n">queue_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">progress_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="p">(</span><span class="n">queue_factor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>

        <span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker_loop</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)]</span>
        <span class="n">unfinished_worker_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">job_producer</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># make interrupting the process with ctrl+c easier</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="n">example_count</span><span class="p">,</span> <span class="n">trained_word_count</span><span class="p">,</span> <span class="n">raw_word_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">word_count</span>
        <span class="n">start</span><span class="p">,</span> <span class="n">next_report</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.00001</span><span class="p">,</span> <span class="mf">1.0</span>

        <span class="k">while</span> <span class="n">unfinished_worker_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">report</span> <span class="o">=</span> <span class="n">progress_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>  <span class="c1"># blocks if workers too slow</span>
            <span class="k">if</span> <span class="n">report</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># a thread reporting that it finished</span>
                <span class="n">unfinished_worker_count</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;worker thread finished; awaiting finish of </span><span class="si">%i</span><span class="s2"> more threads&quot;</span><span class="p">,</span> <span class="n">unfinished_worker_count</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="n">examples</span><span class="p">,</span> <span class="n">trained_words</span><span class="p">,</span> <span class="n">raw_words</span> <span class="o">=</span> <span class="n">report</span>
            <span class="n">job_tally</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># update progress stats</span>
            <span class="n">example_count</span> <span class="o">+=</span> <span class="n">examples</span>
            <span class="n">trained_word_count</span> <span class="o">+=</span> <span class="n">trained_words</span>  <span class="c1"># only words in vocab &amp; sampled</span>
            <span class="n">raw_word_count</span> <span class="o">+=</span> <span class="n">raw_words</span>

            <span class="c1"># log progress once every report_delay seconds</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;=</span> <span class="n">next_report</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">total_examples</span><span class="p">:</span>
                    <span class="c1"># examples-based progress %</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;PROGRESS: at </span><span class="si">%.2f%%</span><span class="s2"> examples, </span><span class="si">%.0f</span><span class="s2"> words/s, in_qsize </span><span class="si">%i</span><span class="s2">, out_qsize </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="mf">100.0</span> <span class="o">*</span> <span class="n">example_count</span> <span class="o">/</span> <span class="n">total_examples</span><span class="p">,</span> <span class="n">trained_word_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">,</span>
                        <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">job_queue</span><span class="p">),</span> <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">progress_queue</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># words-based progress %</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;PROGRESS: at </span><span class="si">%.2f%%</span><span class="s2"> words, </span><span class="si">%.0f</span><span class="s2"> words/s, in_qsize </span><span class="si">%i</span><span class="s2">, out_qsize </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="mf">100.0</span> <span class="o">*</span> <span class="n">raw_word_count</span> <span class="o">/</span> <span class="n">total_words</span><span class="p">,</span> <span class="n">trained_word_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">,</span>
                        <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">job_queue</span><span class="p">),</span> <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">progress_queue</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">next_report</span> <span class="o">=</span> <span class="n">elapsed</span> <span class="o">+</span> <span class="n">report_delay</span>

        <span class="c1"># all done; report the final stats</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;training on </span><span class="si">%i</span><span class="s2"> raw words (</span><span class="si">%i</span><span class="s2"> effective words) took </span><span class="si">%.1f</span><span class="s2">s, </span><span class="si">%.0f</span><span class="s2"> effective words/s&quot;</span><span class="p">,</span>
            <span class="n">raw_word_count</span><span class="p">,</span> <span class="n">trained_word_count</span><span class="p">,</span> <span class="n">elapsed</span><span class="p">,</span> <span class="n">trained_word_count</span> <span class="o">/</span> <span class="n">elapsed</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">job_tally</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;under 10 jobs per worker: consider setting a smaller `batch_words&#39; for smoother alpha decay&quot;</span><span class="p">)</span>

        <span class="c1"># check that the input corpus hasn&#39;t changed during iteration</span>
        <span class="k">if</span> <span class="n">total_examples</span> <span class="ow">and</span> <span class="n">total_examples</span> <span class="o">!=</span> <span class="n">example_count</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;supplied example count (</span><span class="si">%i</span><span class="s2">) did not equal expected count (</span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">example_count</span><span class="p">,</span> <span class="n">total_examples</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total_words</span> <span class="ow">and</span> <span class="n">total_words</span> <span class="o">!=</span> <span class="n">raw_word_count</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;supplied raw word count (</span><span class="si">%i</span><span class="s2">) did not equal expected count (</span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">raw_word_count</span><span class="p">,</span> <span class="n">total_words</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_count</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># number of times train() has been called</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_train_time</span> <span class="o">+=</span> <span class="n">elapsed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clear_sims</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">trained_word_count</span>

    <span class="c1"># basics copied from the train() function</span>
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">total_sentences</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">queue_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">report_delay</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Score the log probability for a sequence of sentences (can be a once-only generator stream).</span>
<span class="sd">        Each sentence must be a list of unicode strings.</span>
<span class="sd">        This does not change the fitted model in any way (see Word2Vec.train() for that).</span>

<span class="sd">        We have currently only implemented score for the hierarchical softmax scheme,</span>
<span class="sd">        so you need to have run word2vec with hs=1 and negative=0 for this to work.</span>

<span class="sd">        Note that you should specify total_sentences; we&#39;ll run into problems if you ask to</span>
<span class="sd">        score more than this number of sentences but it is inefficient to set the value too high.</span>

<span class="sd">        See the article by [taddy]_ and the gensim demo at [deepir]_ for examples of how to use such scores in document classification.</span>

<span class="sd">        .. [taddy] Taddy, Matt.  Document Classification by Inversion of Distributed Language Representations, in Proceedings of the 2015 Conference of the Association of Computational Linguistics.</span>
<span class="sd">        .. [deepir] https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">FAST_VERSION</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;C extension compilation failed, scoring will be slow. &quot;</span>
                <span class="s2">&quot;Install a C compiler and reinstall gensim for fastness.&quot;</span>
            <span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;scoring sentences with </span><span class="si">%i</span><span class="s2"> workers on </span><span class="si">%i</span><span class="s2"> vocabulary and </span><span class="si">%i</span><span class="s2"> features, &quot;</span>
            <span class="s2">&quot;using sg=</span><span class="si">%s</span><span class="s2"> hs=</span><span class="si">%s</span><span class="s2"> sample=</span><span class="si">%s</span><span class="s2"> and negative=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;you must first build vocabulary before scoring new data&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;We have currently only implemented score for the hierarchical softmax scheme, &quot;</span>
                <span class="s2">&quot;so you need to have run word2vec with hs=1 and negative=0 for this to work.&quot;</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">worker_loop</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Compute log probability for each sentence, lifting lists of sentences from the jobs queue.&quot;&quot;&quot;</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># for sg hs, we actually only need one memory loc (running sum)</span>
            <span class="n">neu1</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">job</span> <span class="o">=</span> <span class="n">job_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">job</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># signal to finish</span>
                    <span class="k">break</span>
                <span class="n">ns</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">sentence_id</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">job</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">sentence_id</span> <span class="o">&gt;=</span> <span class="n">total_sentences</span><span class="p">:</span>
                        <span class="k">break</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">:</span>
                        <span class="n">score</span> <span class="o">=</span> <span class="n">score_sentence_sg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">score</span> <span class="o">=</span> <span class="n">score_sentence_cbow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="p">,</span> <span class="n">neu1</span><span class="p">)</span>
                    <span class="n">sentence_scores</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                    <span class="n">ns</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">progress_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span>  <span class="c1"># report progress</span>

        <span class="n">start</span><span class="p">,</span> <span class="n">next_report</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">(),</span> <span class="mf">1.0</span>
        <span class="c1"># buffer ahead only a limited number of jobs.. this is the reason we can&#39;t simply use ThreadPool :(</span>
        <span class="n">job_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="n">queue_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">progress_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="p">(</span><span class="n">queue_factor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>

        <span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker_loop</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># make interrupting the process with ctrl+c easier</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="n">sentence_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">sentence_scores</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="n">total_sentences</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>

        <span class="n">push_done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">done_jobs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">jobs_source</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">grouper</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">chunksize</span><span class="p">))</span>

        <span class="c1"># fill jobs queue with (id, sentence) job items</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">job_no</span><span class="p">,</span> <span class="n">items</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">jobs_source</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">job_no</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunksize</span> <span class="o">&gt;</span> <span class="n">total_sentences</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;terminating after </span><span class="si">%i</span><span class="s2"> sentences (set higher total_sentences if you want more).&quot;</span><span class="p">,</span>
                        <span class="n">total_sentences</span>
                    <span class="p">)</span>
                    <span class="n">job_no</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="k">raise</span> <span class="ne">StopIteration</span><span class="p">()</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;putting job #</span><span class="si">%i</span><span class="s2"> in the queue&quot;</span><span class="p">,</span> <span class="n">job_no</span><span class="p">)</span>
                <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;reached end of input; waiting to finish </span><span class="si">%i</span><span class="s2"> outstanding jobs&quot;</span><span class="p">,</span> <span class="n">job_no</span> <span class="o">-</span> <span class="n">done_jobs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">):</span>
                    <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># give the workers heads up that they can finish -- no more work!</span>
                <span class="n">push_done</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">while</span> <span class="n">done_jobs</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">job_no</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">push_done</span><span class="p">:</span>
                    <span class="n">ns</span> <span class="o">=</span> <span class="n">progress_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">push_done</span><span class="p">)</span>  <span class="c1"># only block after all jobs pushed</span>
                    <span class="n">sentence_count</span> <span class="o">+=</span> <span class="n">ns</span>
                    <span class="n">done_jobs</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
                    <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;=</span> <span class="n">next_report</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="s2">&quot;PROGRESS: at </span><span class="si">%.2f%%</span><span class="s2"> sentences, </span><span class="si">%.0f</span><span class="s2"> sentences/s&quot;</span><span class="p">,</span>
                            <span class="mf">100.0</span> <span class="o">*</span> <span class="n">sentence_count</span><span class="p">,</span> <span class="n">sentence_count</span> <span class="o">/</span> <span class="n">elapsed</span>
                        <span class="p">)</span>
                        <span class="n">next_report</span> <span class="o">=</span> <span class="n">elapsed</span> <span class="o">+</span> <span class="n">report_delay</span>  <span class="c1"># don&#39;t flood log, wait report_delay seconds</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># loop ended by job count; really done</span>
                    <span class="k">break</span>
            <span class="k">except</span> <span class="n">Empty</span><span class="p">:</span>
                <span class="k">pass</span>  <span class="c1"># already out of loop; continue to next push</span>

        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clear_sims</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;scoring </span><span class="si">%i</span><span class="s2"> sentences took </span><span class="si">%.1f</span><span class="s2">s, </span><span class="si">%.0f</span><span class="s2"> sentences/s&quot;</span><span class="p">,</span>
            <span class="n">sentence_count</span><span class="p">,</span> <span class="n">elapsed</span><span class="p">,</span> <span class="n">sentence_count</span> <span class="o">/</span> <span class="n">elapsed</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sentence_scores</span><span class="p">[:</span><span class="n">sentence_count</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">clear_sims</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Removes all L2-normalized vectors for words from the model.</span>
<span class="sd">        You will have to recompute them using init_sims method.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Copy all the existing weights, and reset the weights for the newly</span>
<span class="sd">        added vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;updating layer weights&quot;</span><span class="p">)</span>
        <span class="n">gained_vocab</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">)</span>
        <span class="n">newsyn0</span> <span class="o">=</span> <span class="n">empty</span><span class="p">((</span><span class="n">gained_vocab</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>

        <span class="c1"># randomize the remaining words</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)):</span>
            <span class="c1"># construct deterministic seed from word AND seed argument</span>
            <span class="n">newsyn0</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seeded_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">))</span>

        <span class="c1"># Raise an error if an online update is run before initial training on a corpus</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;You cannot do an online vocabulary-update of a model which has no prior vocabulary. &quot;</span>
                <span class="s2">&quot;First build the vocabulary of your model with a corpus before doing an online update.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">,</span> <span class="n">newsyn0</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn1</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">syn1</span><span class="p">,</span> <span class="n">zeros</span><span class="p">((</span><span class="n">gained_vocab</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn1neg</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">,</span> <span class="n">zeros</span><span class="p">((</span><span class="n">gained_vocab</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># do not suppress learning for already learned words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># zeros suppress learning</span>

    <span class="k">def</span> <span class="nf">reset_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;resetting layer weights&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span> <span class="o">=</span> <span class="n">empty</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="c1"># randomize weights vector by vector, rather than materializing a huge random matrix in RAM at once</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)):</span>
            <span class="c1"># construct deterministic seed from word AND seed argument</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seeded_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn1</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn1neg</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># zeros suppress learning</span>

    <span class="k">def</span> <span class="nf">seeded_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed_string</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create one &#39;random&#39; vector (but deterministic by seed_string)&quot;&quot;&quot;</span>
        <span class="c1"># Note: built-in hash() may vary by Python version or even (in Py3.x) per launch</span>
        <span class="n">once</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hashfxn</span><span class="p">(</span><span class="n">seed_string</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xffffffff</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">once</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span>

    <span class="k">def</span> <span class="nf">intersect_word2vec_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">lockf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Merge the input-hidden weight matrix from the original C word2vec-tool format</span>
<span class="sd">        given, where it intersects with the current vocabulary. (No words are added to the</span>
<span class="sd">        existing vocabulary, but intersecting words adopt the file&#39;s weights, and</span>
<span class="sd">        non-intersecting words are left alone.)</span>

<span class="sd">        `binary` is a boolean indicating whether the data is in binary word2vec format.</span>

<span class="sd">        `lockf` is a lock-factor value to be set for any imported word-vectors; the</span>
<span class="sd">        default value of 0.0 prevents further updating of the vector during subsequent</span>
<span class="sd">        training. Use 1.0 to allow further training updates of merged vectors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">overlap_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading projection weights from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
            <span class="n">header</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">)</span>
            <span class="n">vocab_size</span><span class="p">,</span> <span class="n">vector_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">header</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>  <span class="c1"># throws for invalid file format</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">vector_size</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;incompatible vector size </span><span class="si">%d</span><span class="s2"> in file </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">vector_size</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span>
                <span class="c1"># TOCONSIDER: maybe mismatched vectors still useful enough to merge (truncating/padding)?</span>
            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">binary_len</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span> <span class="o">*</span> <span class="n">vector_size</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
                    <span class="c1"># mixed text and binary: read text first, then binary</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                        <span class="n">ch</span> <span class="o">=</span> <span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">==</span> <span class="sa">b</span><span class="s1">&#39; &#39;</span><span class="p">:</span>
                            <span class="k">break</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">!=</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">:</span>  <span class="c1"># ignore newlines in front of words (some binary files have)</span>
                            <span class="n">word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="sa">b</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">)</span>
                    <span class="n">weights</span> <span class="o">=</span> <span class="n">fromstring</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">binary_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                        <span class="n">overlap_count</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">lockf</span>  <span class="c1"># lock-factor: 0.0 stops further changes</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line_no</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fin</span><span class="p">):</span>
                    <span class="n">parts</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">!=</span> <span class="n">vector_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;invalid vector on line </span><span class="si">%s</span><span class="s2"> (is this really the text format?)&quot;</span> <span class="o">%</span> <span class="n">line_no</span><span class="p">)</span>
                    <span class="n">word</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">REAL</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
                    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                        <span class="n">overlap_count</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">lockf</span>  <span class="c1"># lock-factor: 0.0 stops further changes</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;merged </span><span class="si">%d</span><span class="s2"> vectors into </span><span class="si">%s</span><span class="s2"> matrix from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">overlap_count</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">most_similar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indexer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.most_similar() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.most_similar`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">topn</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="p">,</span> <span class="n">indexer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wmdistance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">document1</span><span class="p">,</span> <span class="n">document2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.wmdistance() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.wmdistance`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">wmdistance</span><span class="p">(</span><span class="n">document1</span><span class="p">,</span> <span class="n">document2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">most_similar_cosmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.most_similar_cosmul() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.most_similar_cosmul`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar_cosmul</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">topn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">similar_by_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.similar_by_word() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.similar_by_word`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">topn</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">similar_by_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.similar_by_vector() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.similar_by_vector`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">topn</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">doesnt_match</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.doesnt_match() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.doesnt_match`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.__getitem__() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.__getitem__`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.__contains__() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.__contains__`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="fm">__contains__</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.similarity() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.similarity`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">n_similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ws1</span><span class="p">,</span> <span class="n">ws2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.n_similarity() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.n_similarity`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">(</span><span class="n">ws1</span><span class="p">,</span> <span class="n">ws2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_output_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_words_list</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Report the probability distribution of the center word given the context words as input to the trained model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;We have currently only implemented predict_output_word for the negative sampling scheme, &quot;</span>
                <span class="s2">&quot;so you need to have run word2vec with negative &gt; 0 for this to work.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> <span class="s1">&#39;syn0&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn1neg&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Parameters required for predicting the output words not found.&quot;</span><span class="p">)</span>

        <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context_words_list</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">word_vocabs</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;All the input context words are out-of-vocabulary for the current model.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">word2_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">index</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_vocabs</span><span class="p">]</span>

        <span class="n">l1</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2_indices</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word2_indices</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbow_mean</span><span class="p">:</span>
            <span class="n">l1</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2_indices</span><span class="p">)</span>

        <span class="n">prob_values</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn1neg</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>  <span class="c1"># propagate hidden -&gt; output and take softmax to get probabilities</span>
        <span class="n">prob_values</span> <span class="o">/=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prob_values</span><span class="p">)</span>
        <span class="n">top_indices</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">prob_values</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">index1</span><span class="p">],</span> <span class="n">prob_values</span><span class="p">[</span><span class="n">index1</span><span class="p">])</span> <span class="k">for</span> <span class="n">index1</span> <span class="ow">in</span> <span class="n">top_indices</span><span class="p">]</span>  <span class="c1"># returning the most probable output words with their probabilities</span>

    <span class="k">def</span> <span class="nf">init_sims</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        init_sims() resides in KeyedVectors because it deals with syn0 mainly, but because syn1 is not an attribute</span>
<span class="sd">        of KeyedVectors, it has to be deleted in this class, and the normalizing of syn0 happens inside of KeyedVectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">replace</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn1&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">init_sims</span><span class="p">(</span><span class="n">replace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">estimate_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Estimate required memory for a model using current settings and provided vocabulary size.&quot;&quot;&quot;</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
        <span class="n">report</span> <span class="o">=</span> <span class="n">report</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="n">report</span><span class="p">[</span><span class="s1">&#39;vocab&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="p">(</span><span class="mi">700</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span> <span class="k">else</span> <span class="mi">500</span><span class="p">)</span>
        <span class="n">report</span><span class="p">[</span><span class="s1">&#39;syn0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span> <span class="o">*</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="n">report</span><span class="p">[</span><span class="s1">&#39;syn1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span> <span class="o">*</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="n">report</span><span class="p">[</span><span class="s1">&#39;syn1neg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span> <span class="o">*</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
        <span class="n">report</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">report</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;estimated required memory for </span><span class="si">%i</span><span class="s2"> words and </span><span class="si">%i</span><span class="s2"> dimensions: </span><span class="si">%i</span><span class="s2"> bytes&quot;</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">,</span> <span class="n">report</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">report</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">log_accuracy</span><span class="p">(</span><span class="n">section</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">log_accuracy</span><span class="p">(</span><span class="n">section</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">questions</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">most_similar</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">case_insensitive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">most_similar</span> <span class="o">=</span> <span class="n">most_similar</span> <span class="ow">or</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">most_similar</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">questions</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="p">,</span> <span class="n">most_similar</span><span class="p">,</span> <span class="n">case_insensitive</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">log_evaluate_word_pairs</span><span class="p">(</span><span class="n">pearson</span><span class="p">,</span> <span class="n">spearman</span><span class="p">,</span> <span class="n">oov</span><span class="p">,</span> <span class="n">pairs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.log_evaluate_word_pairs() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.log_evaluate_word_pairs`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">log_evaluate_word_pairs</span><span class="p">(</span><span class="n">pearson</span><span class="p">,</span> <span class="n">spearman</span><span class="p">,</span> <span class="n">oov</span><span class="p">,</span> <span class="n">pairs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">evaluate_word_pairs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="mi">300000</span><span class="p">,</span> <span class="n">case_insensitive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dummy4unknown</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deprecated. Use self.wv.evaluate_word_pairs() instead.</span>
<span class="sd">        Refer to the documentation for `gensim.models.KeyedVectors.evaluate_word_pairs`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">evaluate_word_pairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="p">,</span> <span class="n">case_insensitive</span><span class="p">,</span> <span class="n">dummy4unknown</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">(vocab=</span><span class="si">%s</span><span class="s2">, size=</span><span class="si">%s</span><span class="s2">, alpha=</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_minimize_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_syn1</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">save_syn1neg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">save_syn0_lockf</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;This method would be deprecated in the future. &quot;</span>
            <span class="s2">&quot;Keep just_word_vectors = model.wv to retain just the KeyedVectors instance &quot;</span>
            <span class="s2">&quot;for read-only querying of word vectors.&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">save_syn1</span> <span class="ow">and</span> <span class="n">save_syn1neg</span> <span class="ow">and</span> <span class="n">save_syn0_lockf</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn1&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">save_syn1</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn1</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn1neg&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">save_syn1neg</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn1neg</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn0_lockf&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">save_syn0_lockf</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_trimmed_post_training</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">delete_temporary_training_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replace_word_vectors_with_normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Discard parameters that are used in training and score. Use if you&#39;re sure you&#39;re done training a model.</span>
<span class="sd">        If `replace_word_vectors_with_normalized` is set, forget the original vectors and only keep the normalized</span>
<span class="sd">        ones = saves lots of memory!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">replace_word_vectors_with_normalized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_sims</span><span class="p">(</span><span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># don&#39;t bother storing the cached normalized vectors, recalculable table</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;ignore&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;syn0norm&#39;</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">,</span> <span class="s1">&#39;cum_table&#39;</span><span class="p">])</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">save</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">SaveLoad</span><span class="o">.</span><span class="n">save</span><span class="o">.</span><span class="vm">__doc__</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># update older models</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">):</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">)</span>  <span class="c1"># discard in favor of cum_table</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> <span class="s1">&#39;index2word&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">make_cum_table</span><span class="p">()</span>  <span class="c1"># rebuild cum_table from vocabulary</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;corpus_count&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s1">&#39;sample_int&#39;</span><span class="p">):</span>
                <span class="k">break</span>  <span class="c1"># already 0.12.0+ style int probabilities</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s1">&#39;sample_probability&#39;</span><span class="p">):</span>
                <span class="n">v</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">sample_probability</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">))</span>
                <span class="k">del</span> <span class="n">v</span><span class="o">.</span><span class="n">sample_probability</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;syn0_lockf&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;syn0&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn0_lockf</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;train_count&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train_count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">model</span><span class="o">.</span><span class="n">total_train_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">_load_specials</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_load_specials</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># loading from a pre-KeyedVectors word2vec model</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;wv&#39;</span><span class="p">):</span>
            <span class="n">wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="p">()</span>
            <span class="n">wv</span><span class="o">.</span><span class="n">syn0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;syn0&#39;</span><span class="p">,</span> <span class="p">[])</span>
            <span class="n">wv</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;syn0norm&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">wv</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;vocab&#39;</span><span class="p">,</span> <span class="p">{})</span>
            <span class="n">wv</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;index2word&#39;</span><span class="p">,</span> <span class="p">[])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">wv</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_word2vec_format</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">fvocab</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">,</span>
                         <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="n">REAL</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">DeprecationWarning</span><span class="p">(</span><span class="s2">&quot;Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_word2vec_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">fvocab</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Deprecated. Use model.wv.save_word2vec_format instead.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">DeprecationWarning</span><span class="p">(</span><span class="s2">&quot;Deprecated. Use model.wv.save_word2vec_format instead.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_latest_training_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_training_loss</span>


<span class="k">class</span> <span class="nc">BrownCorpus</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Iterate over sentences from the Brown corpus (part of NLTK data).&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dirname</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dirname</span> <span class="o">=</span> <span class="n">dirname</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirname</span><span class="p">):</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirname</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="c1"># each file line is a single sentence in the Brown corpus</span>
                <span class="c1"># each token is WORD/POS_TAG</span>
                <span class="n">token_tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
                <span class="c1"># ignore words with non-alphabetic tags like &quot;,&quot;, &quot;!&quot; etc (punctuation, weird stuff)</span>
                <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">tag</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">token_tags</span> <span class="k">if</span> <span class="n">tag</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">words</span><span class="p">:</span>  <span class="c1"># don&#39;t bother sending out empty sentences</span>
                    <span class="k">continue</span>
                <span class="k">yield</span> <span class="n">words</span>


<span class="k">class</span> <span class="nc">Text8Corpus</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Iterate over sentences from the &quot;text8&quot; corpus, unzipped from http://mattmahoney.net/dc/text8.zip .&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">max_sentence_length</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fname</span> <span class="o">=</span> <span class="n">fname</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="n">max_sentence_length</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># the entire corpus is one gigantic line -- there are no sentence marks at all</span>
        <span class="c1"># so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens</span>
        <span class="n">sentence</span><span class="p">,</span> <span class="n">rest</span> <span class="o">=</span> <span class="p">[],</span> <span class="sa">b</span><span class="s1">&#39;&#39;</span>
        <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">rest</span> <span class="o">+</span> <span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">8192</span><span class="p">)</span>  <span class="c1"># avoid loading the entire file (=1 line) into RAM</span>
                <span class="k">if</span> <span class="n">text</span> <span class="o">==</span> <span class="n">rest</span><span class="p">:</span>  <span class="c1"># EOF</span>
                    <span class="n">words</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                    <span class="n">sentence</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>  <span class="c1"># return the last chunk of words, too (may be shorter/longer)</span>
                    <span class="k">if</span> <span class="n">sentence</span><span class="p">:</span>
                        <span class="k">yield</span> <span class="n">sentence</span>
                    <span class="k">break</span>
                <span class="n">last_token</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="sa">b</span><span class="s1">&#39; &#39;</span><span class="p">)</span>  <span class="c1"># last token may have been split in two... keep for next iteration</span>
                <span class="n">words</span><span class="p">,</span> <span class="n">rest</span> <span class="o">=</span> <span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="n">last_token</span><span class="p">])</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span>
                               <span class="n">text</span><span class="p">[</span><span class="n">last_token</span><span class="p">:]</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="k">if</span> <span class="n">last_token</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">([],</span> <span class="n">text</span><span class="p">)</span>
                <span class="n">sentence</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
                <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">sentence</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                    <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">:]</span>


<span class="k">class</span> <span class="nc">LineSentence</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple format: one sentence = one line; words already preprocessed and separated by whitespace.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">max_sentence_length</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `source` can be either a string or a file object. Clip the file to the first</span>
<span class="sd">        `limit` lines (or no clipped if limit is None, the default).</span>

<span class="sd">        Example::</span>

<span class="sd">            sentences = LineSentence(&#39;myfile.txt&#39;)</span>

<span class="sd">        Or for compressed files::</span>

<span class="sd">            sentences = LineSentence(&#39;compressed_text.txt.bz2&#39;)</span>
<span class="sd">            sentences = LineSentence(&#39;compressed_text.txt.gz&#39;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">source</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="n">max_sentence_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">limit</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterate through the lines in the source.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Assume it is a file-like object and try treating it as such</span>
            <span class="c1"># Things that don&#39;t have seek will trigger an exception</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">):</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">line</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="c1"># If it didn&#39;t work like a file, use it as a string filename</span>
            <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="n">fin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">):</span>
                    <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
                        <span class="k">yield</span> <span class="n">line</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span>


<span class="k">class</span> <span class="nc">PathLineSentences</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple format: one sentence = one line; words already preprocessed and separated by whitespace.</span>
<span class="sd">    Like LineSentence, but will process all files in a directory in alphabetical order by filename</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">max_sentence_length</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `source` should be a path to a directory (as a string) where all files can be opened by the</span>
<span class="sd">        LineSentence class. Each file will be read up to</span>
<span class="sd">        `limit` lines (or no clipped if limit is None, the default).</span>

<span class="sd">        Example::</span>

<span class="sd">            sentences = PathLineSentences(os.getcwd() + &#39;\\corpus\\&#39;)</span>

<span class="sd">        The files in the directory should be either text files, .bz2 files, or .gz files.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">source</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="n">max_sentence_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">limit</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;single file read, better to use models.word2vec.LineSentence&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_files</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">]</span>  <span class="c1"># force code compatibility with list of files</span>
        <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>  <span class="c1"># ensures os-specific slash at end of path</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;reading directory </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_files</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">+</span> <span class="n">file</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_files</span><span class="p">]</span>  <span class="c1"># make full paths</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_files</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>  <span class="c1"># makes sure it happens in filename order</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># not a file or a directory, then we can&#39;t do anything with it</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;input is neither a file nor a path&#39;</span><span class="p">)</span>

        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;files read into PathLineSentences:</span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_files</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;iterate through the files&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_files</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;reading file </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="n">fin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">):</span>
                    <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
                        <span class="k">yield</span> <span class="n">line</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span>


<span class="c1"># Example: ./word2vec.py -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">argparse</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
        <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> : </span><span class="si">%(threadName)s</span><span class="s1"> : </span><span class="si">%(levelname)s</span><span class="s1"> : </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;running </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">))</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;using optimization </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">FAST_VERSION</span><span class="p">)</span>

    <span class="c1"># check and process cmdline input</span>
    <span class="n">program</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="nb">globals</span><span class="p">()[</span><span class="s1">&#39;__doc__&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="nb">locals</span><span class="p">())</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="kn">from</span> <span class="nn">gensim.models.word2vec</span> <span class="k">import</span> <span class="n">Word2Vec</span>  <span class="c1"># noqa:F811 avoid referencing __main__ in pickle</span>

    <span class="n">seterr</span><span class="p">(</span><span class="nb">all</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">)</span>  <span class="c1"># don&#39;t ignore numpy errors</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-train&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use text data from file TRAIN to train the model&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-output&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use file OUTPUT to save the resulting word vectors&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-window&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Set max skip length WINDOW between words; default is 5&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-size&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Set size of word vectors; default is 100&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-sample&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Set threshold for occurrence of words. Those that appear with higher frequency in the training data will be randomly down-sampled; default is 1e-3, useful range is (0, 1e-5)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-hs&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use Hierarchical Softmax; default is 0 (not used)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-negative&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of negative examples; default is 5, common values are 3 - 10 (0 = not used)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-threads&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use THREADS threads (default 12)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-iter&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Run more training iterations (default 5)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-min_count&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This will discard words that appear less than MIN_COUNT times; default is 5&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-cbow&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use the continuous bag of words model; default is 1 (use 0 for skip-gram model)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-binary&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Save the resulting vectors in binary mode; default is 0 (off)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-accuracy&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use questions from file ACCURACY to evaluate the model&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cbow</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">skipgram</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">skipgram</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">corpus</span> <span class="o">=</span> <span class="n">LineSentence</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span>
        <span class="n">corpus</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">min_count</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">threads</span><span class="p">,</span>
        <span class="n">window</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">window</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">skipgram</span><span class="p">,</span> <span class="n">hs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">hs</span><span class="p">,</span>
        <span class="n">negative</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">negative</span><span class="p">,</span> <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">iter</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
        <span class="n">outfile</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">output</span>
        <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">outfile</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">outfile</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">outfile</span> <span class="o">+</span> <span class="s1">&#39;.model&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">binary</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">outfile</span> <span class="o">+</span> <span class="s1">&#39;.model.bin&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">outfile</span> <span class="o">+</span> <span class="s1">&#39;.model.txt&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">accuracy</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">accuracy</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;finished running </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">program</span><span class="p">)</span>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, FIT.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>