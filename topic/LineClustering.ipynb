{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, random, re\n",
    "pfile = open('saved/lyrics_filtered.pkl', 'rb')\n",
    "\n",
    "lyrics = pickle.load(pfile)\n",
    "random.shuffle(lyrics)\n",
    "\n",
    "dev_lyrics = lyrics[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import thulac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "class Doc():\n",
    "    \n",
    "    model = Word2Vec.load('saved/word2vec_model')\n",
    "    cut = thulac.thulac(seg_only=True)  #只进行分词，不进行词性标注\\n\"\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    def __init__(self, document, tokenizer = 'char'):\n",
    "        '''Doc class, a representation of document.\n",
    "        \n",
    "        @param document: A Chinese sentence.\n",
    "        @param tokenizer: the tokenizer(char/word)\n",
    "        '''\n",
    "        if tokenizer == 'char':\n",
    "            self.bag_of_words = list(document)\n",
    "        if tokenizer == 'word':\n",
    "            self.bag_of_words = list(list(zip(*Doc.cut.cut(sentence)))[0])\n",
    "        \n",
    "        self.vec = self.to_vec()\n",
    "        \n",
    "        Doc.corpus.append(self)\n",
    "    \n",
    "    def to_vec(self):\n",
    "        vec = np.zeros(512, dtype='float')\n",
    "        for word in self.bag_of_words:\n",
    "            if word in Doc.model.wv:\n",
    "                vec += Doc.model.wv[word]\n",
    "            else:\n",
    "                for char in word:\n",
    "                    vec += Doc.model.wv[char] if char in Doc.model.wv else 0\n",
    "\n",
    "        return vec / np.linalg.norm(vec) if np.linalg.norm(vec) > 0 else vec\n",
    "\n",
    "    def similarity(doc1, doc2):\n",
    "        '''Return the cosine distance between two sentences.'''\n",
    "        return np.dot(doc1.vec, doc2.vec)\n",
    "\n",
    "    def most_similar(self):\n",
    "        '''Find the most similar sentence in the corpus.\n",
    "\n",
    "        Similar defined as cosine distance.\n",
    "        '''\n",
    "        most_simi, winner = 0, Doc('')\n",
    "\n",
    "        for i, candidate in enumerate(Doc.corpus):\n",
    "            simi = Doc.similarity(self, candidate)\n",
    "            if simi > most_simi and candidate.bag_of_words != self.bag_of_words:\n",
    "                most_simi, winner = simi, candidate\n",
    "\n",
    "        return most_simi, ''.join(winner.bag_of_words)\n",
    "    \n",
    "    def test():\n",
    "        '''Unit test & usage'''\n",
    "        sentence1 = '天青色等烟雨'\n",
    "        doc1 = Doc(sentence1)\n",
    "        print('Tokenized and word vec[:10] of %s:' % sentence1)\n",
    "        print(doc1.bag_of_words)\n",
    "        print(doc1.to_vec()[:10])\n",
    "        print('')\n",
    "        print('Most similar word to 河流')\n",
    "        print(Doc.model.most_similar('河流'))\n",
    "        print('')\n",
    "        sentence2 = '而我在等你'\n",
    "        doc2 = Doc(sentence2)\n",
    "        print('Similarity between %s, %s' % (sentence1, sentence2))\n",
    "        print(Doc.similarity(doc1, doc2))\n",
    "        print('')\n",
    "        print('Most similar to %s in corpus' % sentence1)\n",
    "        print(doc1.most_similar())\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics(lyrics):\n",
    "    Doc.corpus = []\n",
    "    for lyric in lyrics:\n",
    "        for sentence in lyric:\n",
    "            Doc(sentence)\n",
    "\n",
    "load_lyrics(dev_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.48571671e-02,   1.93655009e-02,  -7.30190808e-03,\n",
       "        -3.03984456e-02,  -1.63738999e-03,   2.94244422e-02,\n",
       "        -7.13335421e-02,  -7.41774667e-02,   3.93391895e-02,\n",
       "        -2.91719492e-02,   1.86987652e-02,   8.03371543e-03,\n",
       "        -9.36264681e-03,   8.38347216e-03,  -6.52779517e-02,\n",
       "        -3.99518762e-02,   7.29505621e-02,   3.46279187e-03,\n",
       "         6.94428955e-02,  -2.54271927e-02,  -2.16518580e-02,\n",
       "         5.87576212e-03,  -2.85494016e-02,   5.47539963e-02,\n",
       "         3.95933170e-02,   3.48851657e-02,  -4.00025828e-02,\n",
       "         1.60664714e-02,   2.76331733e-02,   7.87805289e-02,\n",
       "        -8.14829098e-02,  -7.25446614e-02,   2.62316425e-02,\n",
       "        -2.40862873e-02,  -1.07238769e-02,   1.53676263e-02,\n",
       "        -2.04014147e-02,  -7.99943282e-02,  -1.49576747e-02,\n",
       "         2.96647055e-02,   8.25912472e-03,   1.14714220e-02,\n",
       "         5.53795345e-03,  -3.53379884e-02,  -4.63778060e-03,\n",
       "        -5.07312944e-02,   8.21440663e-03,   3.44553906e-02,\n",
       "         5.61354592e-02,   1.37802244e-02,  -2.44966008e-03,\n",
       "         7.64052218e-02,   3.80527248e-03,  -8.58484714e-02,\n",
       "        -3.99178616e-02,  -1.50682029e-02,  -1.18756932e-02,\n",
       "         2.72418292e-02,  -3.54322783e-02,  -1.35084900e-02,\n",
       "        -6.28067672e-03,   8.47343784e-02,   1.82145875e-02,\n",
       "         3.03362986e-02,   2.91947528e-03,  -2.65110897e-02,\n",
       "        -4.23387165e-02,   9.89692500e-02,  -1.08250306e-02,\n",
       "         3.96395945e-02,   7.56634170e-02,  -3.03055267e-02,\n",
       "        -4.42792690e-02,   1.13240969e-02,  -3.30991826e-02,\n",
       "        -1.87864358e-02,   1.07269762e-01,  -8.16338559e-02,\n",
       "        -1.07454384e-01,   6.52147080e-02,  -1.55130100e-02,\n",
       "         2.37663180e-02,   2.94332600e-02,  -2.09564888e-02,\n",
       "         4.24643545e-03,  -5.20307842e-02,   4.79573872e-02,\n",
       "        -8.14347609e-02,   4.41158735e-02,  -4.14708498e-02,\n",
       "        -7.09582357e-02,  -1.16179143e-01,  -1.47850269e-02,\n",
       "         1.01858905e-02,  -4.95284666e-02,   3.94906963e-02,\n",
       "        -2.26269181e-02,  -4.04829697e-02,   8.34094368e-02,\n",
       "         2.66342287e-02,  -4.28543129e-02,   4.04052420e-03,\n",
       "         1.87610306e-02,   4.62462973e-02,  -2.49818712e-02,\n",
       "         9.72058633e-02,   5.30446752e-03,   1.11734488e-02,\n",
       "         5.52298223e-02,   6.52920712e-02,  -5.76129096e-03,\n",
       "        -4.62228351e-02,  -1.96179464e-02,   2.16769451e-02,\n",
       "        -1.08522270e-02,   5.10622609e-02,   7.04115335e-03,\n",
       "         2.38648148e-04,   2.94639762e-02,   1.22634606e-02,\n",
       "        -2.45826241e-02,  -3.95060090e-02,   6.48881447e-03,\n",
       "        -4.70593157e-02,  -1.57164526e-02,  -5.13052300e-02,\n",
       "        -2.74566388e-02,  -5.61133162e-02,   2.30121598e-02,\n",
       "         8.85926760e-03,  -3.85437918e-04,   3.68495443e-02,\n",
       "        -5.09608186e-02,  -6.82615756e-03,   4.09010699e-03,\n",
       "        -4.11412504e-04,  -1.24701417e-02,  -4.68750154e-02,\n",
       "        -4.39391752e-02,  -5.18746865e-02,   7.58576231e-02,\n",
       "        -2.26251306e-02,   5.01926613e-02,  -4.29789536e-02,\n",
       "         6.77256561e-02,   3.15464954e-02,  -1.02291954e-02,\n",
       "        -2.29515289e-02,   2.71366925e-02,  -3.22143000e-02,\n",
       "        -3.86627223e-02,  -1.68946428e-02,   2.17100919e-02,\n",
       "        -6.33190035e-04,  -1.01211373e-02,  -1.60468259e-02,\n",
       "         8.09099938e-03,  -5.85212604e-03,   1.76576770e-02,\n",
       "         5.23654724e-02,  -7.96318805e-02,   3.22695423e-02,\n",
       "        -9.33192186e-02,   3.27724815e-02,   2.29464099e-02,\n",
       "        -5.39842912e-03,  -2.58833473e-02,   2.97273576e-02,\n",
       "        -3.44211838e-02,   1.73622746e-02,  -2.50613670e-03,\n",
       "         1.09996958e-01,   1.20692295e-03,  -3.17397168e-02,\n",
       "         6.31687653e-02,   5.92251313e-02,   1.35489663e-02,\n",
       "        -3.94561544e-02,  -3.11024669e-02,  -6.92090241e-04,\n",
       "        -6.24130069e-02,  -3.04390041e-02,   5.43648498e-03,\n",
       "        -1.96273752e-02,  -6.46349442e-03,  -2.47037334e-02,\n",
       "        -3.95175085e-02,   3.92630175e-02,   7.71182588e-02,\n",
       "         2.72087650e-02,   1.91378128e-02,   9.23334689e-02,\n",
       "         1.11496503e-01,  -2.06435749e-02,  -1.47847052e-02,\n",
       "         5.42541723e-02,   4.41165735e-02,  -6.65396511e-02,\n",
       "         1.59453262e-03,   3.55919519e-02,   1.27437575e-02,\n",
       "         7.52765601e-02,  -6.57505731e-02,  -3.83152186e-03,\n",
       "        -1.41082632e-02,  -4.37657110e-02,   3.31249765e-02,\n",
       "        -8.63082144e-02,   2.22032843e-02,   7.53147606e-05,\n",
       "         2.43225762e-02,  -7.99025976e-03,  -1.62233619e-02,\n",
       "         4.52220217e-02,   3.07472299e-03,   1.09470692e-02,\n",
       "        -1.68089702e-02,  -4.00381855e-02,   8.90324382e-03,\n",
       "         2.24978024e-02,   2.29858905e-02,   1.08464849e-03,\n",
       "        -6.86969795e-03,   2.44452131e-02,   2.08117570e-02,\n",
       "        -4.70935192e-02,   7.22410130e-02,   1.79566666e-02,\n",
       "        -2.17551559e-02,  -4.81408805e-02,  -6.08880796e-02,\n",
       "         1.55957430e-02,  -5.65560920e-02,   4.34515650e-02,\n",
       "         3.48052224e-03,   3.56661970e-02,  -4.50378982e-03,\n",
       "         2.24806365e-02,   1.05548441e-02,   2.72010925e-02,\n",
       "         2.10590727e-02,   1.41796777e-02,   2.77046179e-02,\n",
       "         4.65880794e-02,   2.85575860e-02,   2.04204477e-02,\n",
       "        -7.84215511e-02,  -2.36513461e-02,  -9.64234262e-02,\n",
       "        -3.62677801e-02,  -3.63763215e-04,  -2.58099566e-02,\n",
       "        -1.15642622e-02,   9.21440688e-02,  -7.31044626e-02,\n",
       "        -7.87057126e-03,  -6.97650174e-02,   7.55826119e-03,\n",
       "        -6.34729322e-02,  -3.12203906e-02,   2.97334196e-02,\n",
       "         7.23334819e-03,   5.16000071e-02,   4.58941141e-02,\n",
       "         7.63044172e-03,  -8.96509400e-02,   9.65531383e-02,\n",
       "         2.94918761e-02,   3.71854883e-03,  -7.01966378e-02,\n",
       "         1.76669219e-02,  -5.31699215e-02,  -1.11650028e-01,\n",
       "        -1.13714107e-02,   6.27284534e-02,  -3.41976522e-02,\n",
       "         2.65466172e-02,  -7.17106130e-03,   1.88432872e-03,\n",
       "        -3.79561728e-02,   3.69548194e-02,   5.93340455e-03,\n",
       "        -6.65328533e-02,  -7.18872107e-02,  -1.97135416e-02,\n",
       "        -1.27996463e-02,  -2.11475776e-02,  -1.93631553e-02,\n",
       "         6.67509035e-02,  -1.48534124e-02,   7.92129532e-03,\n",
       "        -3.33565977e-02,  -4.85989887e-03,   1.33863135e-02,\n",
       "         2.50004175e-04,   7.76427847e-03,  -4.59850324e-02,\n",
       "        -3.79209751e-02,   3.12979186e-02,  -3.61168132e-02,\n",
       "        -6.30602407e-02,  -2.43475522e-02,   9.62145890e-03,\n",
       "        -1.94683130e-02,   1.25494617e-02,   2.97020776e-02,\n",
       "         3.10149248e-03,   1.84130863e-02,  -3.26178381e-02,\n",
       "         1.92962342e-02,  -2.15525573e-02,   7.05419990e-02,\n",
       "         5.84968368e-02,   7.69341660e-02,  -4.26905361e-02,\n",
       "        -1.83747272e-02,   4.00415026e-02,  -2.36073929e-02,\n",
       "        -2.03845305e-02,   2.52081559e-02,   6.02019104e-02,\n",
       "         6.76827721e-02,  -6.01349501e-02,   2.02373775e-02,\n",
       "         1.85192277e-02,   4.10606729e-02,  -7.58424023e-02,\n",
       "         2.11960080e-02,  -5.87751607e-02,  -1.05182709e-02,\n",
       "         4.97087893e-02,  -3.16889404e-02,  -7.44309172e-02,\n",
       "         3.65583126e-02,   2.20195614e-02,  -5.93470102e-02,\n",
       "         2.52132361e-02,  -3.45888349e-02,   3.51278287e-02,\n",
       "        -6.40099346e-02,   1.18098726e-02,  -2.21475606e-02,\n",
       "         2.01551358e-02,  -5.95275056e-02,   4.08746079e-02,\n",
       "        -2.65827564e-02,   2.01257269e-02,   1.45834824e-02,\n",
       "        -2.80146106e-02,   1.54054453e-02,   4.86832382e-02,\n",
       "         7.73998611e-02,   1.94797707e-02,  -5.54519238e-02,\n",
       "        -2.40609783e-02,   5.67165769e-02,  -6.58765890e-02,\n",
       "        -5.85598723e-02,  -4.01747380e-02,  -9.11080938e-02,\n",
       "        -1.92734331e-02,   9.14310844e-03,   7.36233857e-02,\n",
       "        -9.66095275e-02,  -3.41934271e-02,  -1.38664683e-02,\n",
       "         4.85183908e-02,   3.62365484e-03,  -1.08163952e-02,\n",
       "        -4.39235922e-02,   4.66360094e-03,  -2.48192998e-02,\n",
       "         4.18800677e-02,  -2.18686307e-02,  -4.88910095e-02,\n",
       "         4.91339408e-02,  -4.11937760e-02,   9.94519106e-03,\n",
       "        -1.60776063e-02,   4.65134021e-03,   4.18765650e-02,\n",
       "         1.94428233e-02,   3.76979698e-03,  -7.22954867e-03,\n",
       "         2.08080094e-03,  -3.45505739e-02,   5.69933662e-02,\n",
       "         3.52541981e-02,   1.44495244e-02,   5.37807600e-03,\n",
       "        -1.98907910e-02,   8.71061817e-02,   4.20409049e-02,\n",
       "        -3.34621625e-02,   5.86751269e-03,   4.98628371e-02,\n",
       "        -1.70248226e-02,  -8.75040300e-02,  -4.33428946e-04,\n",
       "        -2.51499893e-02,  -2.23594325e-02,   8.00258749e-03,\n",
       "         6.25607540e-04,   6.64479390e-03,   1.46557645e-02,\n",
       "        -2.20762047e-02,   2.39500969e-02,   3.13773095e-02,\n",
       "        -3.33711891e-02,   5.71037699e-03,   3.08220181e-02,\n",
       "        -1.07272659e-01,  -1.02975902e-02,   3.47019375e-02,\n",
       "         6.90012429e-03,   2.86520049e-02,   5.02925912e-02,\n",
       "         5.59999373e-03,  -5.22253729e-02,  -1.39043104e-02,\n",
       "         2.13618614e-02,   8.02010925e-02,  -9.39607756e-02,\n",
       "         6.71981450e-02,   7.87063341e-02,  -2.87303480e-02,\n",
       "        -3.25746725e-02,  -8.58762114e-02,  -1.87866600e-02,\n",
       "        -4.45341873e-02,  -1.26955055e-02,   6.79361112e-02,\n",
       "         6.86551102e-02,   8.74998867e-03,   3.88942766e-02,\n",
       "         8.31784166e-02,  -1.31406929e-02,   1.24638685e-02,\n",
       "         4.96833831e-02,  -2.08096348e-02,  -5.57774880e-02,\n",
       "         1.99176700e-03,   2.40161787e-02,  -1.15763110e-02,\n",
       "        -5.70789289e-02,  -1.33336840e-01,  -2.08651225e-02,\n",
       "         3.77560687e-02,   1.13329156e-02,   1.20789046e-01,\n",
       "        -1.00412459e-02,   4.87408256e-02,  -9.10773306e-04,\n",
       "        -3.83935578e-02,   3.93801856e-02,   7.06080710e-03,\n",
       "         2.17561530e-02,   1.66778608e-03,  -5.08378319e-04,\n",
       "        -2.85635811e-02,   3.30685351e-02,   3.38203623e-02,\n",
       "        -3.84337267e-02,  -2.73925853e-02,   9.21587233e-03,\n",
       "        -5.04717215e-02,   1.66677913e-02,   3.37124152e-03,\n",
       "         4.84018161e-02,   3.77200339e-02,   3.02439332e-02,\n",
       "         1.88592186e-02,  -6.37029840e-02,   7.82529453e-03,\n",
       "        -8.34112173e-02,  -3.21882935e-02,  -2.11315650e-02,\n",
       "        -9.00018875e-03,  -4.45810885e-02,   2.19079394e-03,\n",
       "        -3.66208116e-02,   5.59743006e-02,  -1.04898446e-03,\n",
       "        -2.06363274e-02,   8.60907757e-02,  -4.48432108e-02,\n",
       "        -2.31489998e-03,  -2.75501846e-03,  -1.52808203e-02,\n",
       "         3.13929692e-02,   4.72910594e-02,   5.51744278e-02,\n",
       "         1.73775112e-01,   4.58762439e-02,  -7.65977822e-02,\n",
       "         4.23823431e-02,   1.14174012e-01,  -3.78587143e-02,\n",
       "         7.43148480e-02,   8.59235884e-02,   6.70488962e-02,\n",
       "        -5.05956809e-02,  -1.06221041e-02,  -4.94036274e-02,\n",
       "         2.01577857e-02,   1.93259767e-02,  -7.18411123e-02,\n",
       "         5.55712268e-02,   8.23400848e-03,  -1.51512706e-02,\n",
       "        -5.00744814e-02,  -2.36839754e-02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc('天青色等烟雨').to_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and word vec[:10] of 天青色等烟雨:\n",
      "['天', '青', '色', '等', '烟', '雨']\n",
      "[-0.01485717  0.0193655  -0.00730191 -0.03039845 -0.00163739  0.02942444\n",
      " -0.07133354 -0.07417747  0.03933919 -0.02917195]\n",
      "\n",
      "Most similar word to 河流\n",
      "[('奔腾', 0.675121545791626), ('稻香', 0.6594686508178711), ('原野', 0.6528366804122925), ('穿行', 0.6512413024902344), ('海洋', 0.6460175514221191), ('流淌啊', 0.6350507140159607), ('山间', 0.6339350938796997), ('旷野', 0.6277040243148804), ('江河', 0.6272121667861938), ('高原', 0.6268514394760132)]\n",
      "\n",
      "Similarity between 天青色等烟雨, 而我在等你\n",
      "0.36849127679\n",
      "\n",
      "Most similar to 天青色等烟雨 in corpus\n",
      "(0.80305124424941721, '树叶飘落在十月 天色刺眼 我却写了阴雨天')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Doc.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD,NMF,LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topic(texts, topic_model, n_topics, vec_model=\"tf\", thr=1e-2, **kwargs):\n",
    "        # 1. vectorization\n",
    "        vectorizer = CountVectorizer() if vec_model== \"tf\" else TfidVectorizer()\n",
    "        text_vec = vectorizer.fit_transform(texts)\n",
    "        words = np.array(vectorizer.get_feature_names())\n",
    "        # 2. topic finding\n",
    "        topic_models = {\"nmf\":NMF, \"svd\": TruncatedSVD, \"lda\":LatentDirichletAllocation,\"kmeans\":KMeans}\n",
    "        topicfinder = topic_models[topic_model](n_topics, **kwargs).fit(text_vec)\n",
    "        topic_dists = topicfinder.components_ if topic_model is not \"kmeans\" else topicfinder.cluster_centers_\n",
    "        topic_dists /= topic_dists.max(axis = 1).reshape((-1,1))\n",
    "        # 3. keywords for topics\n",
    "        def _topic_keywords(topic_dist):\n",
    "            keywords_index = np.abs(topic_dist) >= thr\n",
    "            keywords_prefix = np.where(np.sign(topic_dist)>0, \"\",\"^\")[keywords_index]\n",
    "            keywords = \" | \".join(map(lambda x: \"\".join(x), zip(keywords_prefix, words[keywords_index])))\n",
    "            return keywords\n",
    "        topic_keywords = map(_topic_keywords, topic_dists)\n",
    "        return \"\\n\".join(\"Topic %i:%s\" % (i, t) for i, t in enumerate(topic_keywords))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = vectorizer.fit_transform([\"we love burgers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clearcut_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"],[1000,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:bergers | hate | love | sandwiches | we\n",
      "Topic 1:bergers | ^hate | love | ^sandwiches\n",
      "Topic 2:bergers | hate | love | sandwiches | ^we\n",
      "Topic 3:bergers | ^hate | ^love | sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(generate_clearcut_topics(), \"svd\", 4, vec_model=\"tf\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
