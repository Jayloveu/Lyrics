{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, random, re\n",
    "pfile = open('saved/lyrics_filtered.pkl', 'rb')\n",
    "\n",
    "lyrics = pickle.load(pfile)\n",
    "random.shuffle(lyrics)\n",
    "\n",
    "dev_lyrics = lyrics[:500]\n",
    "test_lyrics = lyrics[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import thulac\n",
    "\n",
    "import many_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "class Doc():\n",
    "    \n",
    "    model = Word2Vec.load('saved/word2vec_model')\n",
    "    cut = thulac.thulac(seg_only=True)  #只进行分词，不进行词性标注\\n\"\n",
    "    \n",
    "    corpus = []\n",
    "    stop = many_stop_words.get_stop_words()\n",
    "    \n",
    "    def __init__(self, document, tokenizer = 'char'):\n",
    "        '''Doc class, a representation of document.\n",
    "        \n",
    "        @param document: A Chinese sentence.\n",
    "        @param tokenizer: the tokenizer(char/word)\n",
    "        '''\n",
    "        document = ''.join([char for char in document if char not in Doc.stop])\n",
    "        if tokenizer == 'char':\n",
    "            self.bag_of_words = list(document)\n",
    "        if tokenizer == 'word':\n",
    "            self.bag_of_words = list(list(zip(*Doc.cut.cut(document)))[0])\n",
    "        \n",
    "        self.vec = self.to_vec()\n",
    "        Doc.corpus.append(self)\n",
    "    \n",
    "    def to_vec(self):\n",
    "        vec = np.zeros([512], dtype='float')\n",
    "        for word in self.bag_of_words:\n",
    "            if word in Doc.model.wv:\n",
    "                vec += Doc.model.wv[word]\n",
    "            else:\n",
    "                for char in word:\n",
    "                    vec += Doc.model.wv[char] if char in Doc.model.wv else 0\n",
    "\n",
    "        return vec / np.linalg.norm(vec) if np.linalg.norm(vec) > 0 else vec\n",
    "\n",
    "    def similarity(self, doc2):\n",
    "        '''Return the cosine distance between two lines.'''\n",
    "        return np.dot(self.vec, doc2.vec)\n",
    "\n",
    "    def most_similar(self):\n",
    "        '''Find the most similar line in the corpus.\n",
    "\n",
    "        Similar defined as cosine distance.\n",
    "        '''\n",
    "        most_simi, winner = 0, Doc('')\n",
    "\n",
    "        for i, candidate in enumerate(Doc.corpus):\n",
    "            simi = Doc.similarity(self, candidate)\n",
    "            if simi > most_simi and candidate.bag_of_words != self.bag_of_words:\n",
    "                most_simi, winner = simi, candidate\n",
    "\n",
    "        return most_simi, ''.join(winner.bag_of_words)\n",
    "    \n",
    "    def load_corpus(lyrics, doc_type='line', tokenizer='char'):\n",
    "        '''Load from a list of lyrics, a list of a list of lines.\n",
    "        \n",
    "        doc_type: line/piece\n",
    "        '''\n",
    "        Doc.corpus = []\n",
    "        for idx, lyric in enumerate(lyrics):\n",
    "            if idx % 100 == 0:\n",
    "                print('Loading %d' % idx)\n",
    "            if doc_type == 'line':\n",
    "                for line in lyric:\n",
    "                    Doc(line, tokenizer)\n",
    "            if doc_type == 'piece':\n",
    "                Doc(''.join(lyric), tokenizer)\n",
    "\n",
    "    def test():\n",
    "        '''Unit test & usage'''\n",
    "        line1 = '天青色等烟雨'\n",
    "        doc1 = Doc(line1)\n",
    "        print('Tokenized and word vec[:10] of %s:' % line1)\n",
    "        print(doc1.bag_of_words)\n",
    "        print(doc1.to_vec()[:10])\n",
    "        print('')\n",
    "        print('Most similar word to 河流')\n",
    "        print(Doc.model.most_similar('河流'))\n",
    "        print('')\n",
    "        line2 = '而我在等你'\n",
    "        doc2 = Doc(line2)\n",
    "        print('Similarity between %s, %s' % (line1, line2))\n",
    "        print(Doc.similarity(doc1, doc2))\n",
    "        print('')\n",
    "        print('Most similar to %s in corpus' % line1)\n",
    "        print(doc1.most_similar())\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0\n",
      "Loading 100\n",
      "Loading 200\n",
      "Loading 300\n",
      "Loading 400\n",
      "Tokenized and word vec[:10] of 天青色等烟雨:\n",
      "['天', '青', '色', '等', '烟', '雨']\n",
      "[-0.01485717  0.0193655  -0.00730191 -0.03039845 -0.00163739  0.02942444\n",
      " -0.07133354 -0.07417747  0.03933919 -0.02917195]\n",
      "\n",
      "Most similar word to 河流\n",
      "[('奔腾', 0.675121545791626), ('稻香', 0.6594686508178711), ('原野', 0.6528366804122925), ('穿行', 0.6512413620948792), ('海洋', 0.6460176110267639), ('流淌啊', 0.6350507140159607), ('山间', 0.6339350938796997), ('旷野', 0.6277040839195251), ('江河', 0.6272122263908386), ('高原', 0.626851499080658)]\n",
      "\n",
      "Similarity between 天青色等烟雨, 而我在等你\n",
      "0.531282287307\n",
      "\n",
      "Most similar to 天青色等烟雨 in corpus\n",
      "(0.6912553735328979, '星星像眼泪坠落大海风处歌声飘传说中语失心女子悲伤系裙摆天天亲吻诺言等待夜夜月徘徊声声哼唱情深奈听都会伤怀晚风常叹气翻山越海找心回天天亲吻诺言等待夜夜月徘徊声声哼唱情深奈听都会伤怀晚风常叹气翻山越海找心回孤单相恋失心女子')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Doc.load_corpus(dev_lyrics, doc_type='piece', tokenizer='word')\n",
    "Doc.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([doc.bag_of_words for doc in Doc.corpus])\n",
    "corpus = [dictionary.doc2bow(doc.bag_of_words) for doc in Doc.corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "ldamodel = Lda(corpus, num_topics=10, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"就\" + 0.011*\" \" + 0.010*\"想\" + 0.010*\"爱\" + 0.008*\"没\" + 0.007*\"要\" + 0.006*\"过\" + 0.006*\"会\" + 0.006*\"里\" + 0.006*\"说\" + 0.006*\"永远\" + 0.006*\"怕\" + 0.005*\"能\" + 0.005*\"呀\" + 0.005*\"出\" + 0.004*\"像\" + 0.004*\"四川\" + 0.004*\"中\" + 0.004*\"心\" + 0.004*\"做\" + 0.004*\"都\" + 0.004*\"梦\" + 0.004*\"地\" + 0.004*\"对\" + 0.004*\"时\" + 0.004*\"走\" + 0.003*\"快乐\" + 0.003*\"故乡\" + 0.003*\"寂寞\" + 0.003*\"见\" + 0.003*\"爱情\" + 0.003*\"啊\" + 0.003*\"办法\" + 0.003*\"才\" + 0.003*\"宝贝\" + 0.003*\"天\" + 0.003*\"开\" + 0.003*\"间\" + 0.003*\"停\" + 0.003*\"家\" + 0.002*\"难\" + 0.002*\"次\" + 0.002*\"幸福\" + 0.002*\"回\" + 0.002*\"相\" + 0.002*\"吹\" + 0.002*\"多少\" + 0.002*\"离\" + 0.002*\"太\" + 0.002*\"希望\" + 0.002*\"痛\" + 0.002*\"山水\" + 0.002*\"花\" + 0.002*\"世界\" + 0.002*\"昨天\" + 0.002*\"离开\" + 0.002*\"偏偏\" + 0.002*\"演唱\" + 0.002*\"温暖\" + 0.002*\"种\" + 0.002*\"愿\" + 0.002*\"隆咚\" + 0.002*\"终点\" + 0.002*\"哈\" + 0.002*\"伤口\" + 0.002*\"美\" + 0.002*\"未\" + 0.002*\"总\" + 0.002*\"听\" + 0.002*\"时空\" + 0.002*\"爹娘\" + 0.002*\"真\" + 0.001*\"等\" + 0.001*\"守\" + 0.001*\"回忆\" + 0.001*\"进\" + 0.001*\"眼泪\" + 0.001*\"注定\" + 0.001*\"问\" + 0.001*\"应\" + 0.001*\"老\" + 0.001*\"处\" + 0.001*\"明天\" + 0.001*\"遍地\" + 0.001*\"出现\" + 0.001*\"风\" + 0.001*\"找\" + 0.001*\"将\" + 0.001*\"留\" + 0.001*\"时间\" + 0.001*\"努力\" + 0.001*\"仿佛\" + 0.001*\"桃花源\" + 0.001*\"天地\" + 0.001*\"妈妈\" + 0.001*\"爱洋葱\" + 0.001*\"情歌\" + 0.001*\"眼睛\" + 0.001*\"路\" + 0.001*\"己\"'),\n",
       " (1,\n",
       "  '0.015*\"要\" + 0.007*\"没\" + 0.006*\"爱\" + 0.006*\"玩\" + 0.006*\"时\" + 0.006*\" \" + 0.006*\"像\" + 0.006*\"地\" + 0.005*\"都\" + 0.004*\"尽情\" + 0.004*\"想\" + 0.004*\"梦\" + 0.004*\"想念\" + 0.004*\"就\" + 0.004*\"做\" + 0.004*\"情\" + 0.004*\"总\" + 0.004*\"能\" + 0.004*\"中\" + 0.003*\"全体\" + 0.003*\"出动\" + 0.003*\"似\" + 0.003*\"就算\" + 0.003*\"更\" + 0.003*\"变\" + 0.003*\"明天\" + 0.003*\"唱\" + 0.003*\"里\" + 0.003*\"多\" + 0.003*\"会\" + 0.003*\"女\" + 0.003*\"心\" + 0.003*\"天\" + 0.003*\"嫁\" + 0.003*\"出\" + 0.003*\"过\" + 0.002*\"地方\" + 0.002*\"永远\" + 0.002*\"雨\" + 0.002*\"将\" + 0.002*\"己\" + 0.002*\"家\" + 0.002*\"尽\" + 0.002*\"共\" + 0.002*\"温暖\" + 0.002*\"歌\" + 0.002*\"飘\" + 0.002*\"亲爱\" + 0.002*\"块\" + 0.002*\"问\" + 0.002*\"美丽\" + 0.002*\"首\" + 0.002*\"埋\" + 0.002*\"快车\" + 0.002*\"握\" + 0.002*\"生命\" + 0.002*\"明白\" + 0.002*\"男\" + 0.002*\"留\" + 0.002*\"手心\" + 0.002*\"石头\" + 0.002*\"未\" + 0.002*\"岁月\" + 0.002*\"大姑娘\" + 0.002*\"天空\" + 0.002*\"全场\" + 0.002*\"般\" + 0.001*\"说\" + 0.001*\"多少\" + 0.001*\"夜\" + 0.001*\"活\" + 0.001*\"心手\" + 0.001*\"浓\" + 0.001*\"黑色\" + 0.001*\"架\" + 0.001*\"相\" + 0.001*\"天会\" + 0.001*\"完场\" + 0.001*\"天炸弹\" + 0.001*\"摇摇\" + 0.001*\"晃晃榜样\" + 0.001*\"撒旦\" + 0.001*\"两\" + 0.001*\"深\" + 0.001*\"才\" + 0.001*\"灿烂\" + 0.001*\"管\" + 0.001*\"走\" + 0.001*\"愿\" + 0.001*\"难\" + 0.001*\"怕\" + 0.001*\"旅程\" + 0.001*\"枯萎\" + 0.001*\"回忆\" + 0.001*\"悸动\" + 0.001*\"运气\" + 0.001*\"错\" + 0.001*\"胡姬花\" + 0.001*\"磨练\" + 0.001*\"忽然\"'),\n",
       " (2,\n",
       "  '0.085*\" \" + 0.015*\"想\" + 0.015*\"爱\" + 0.013*\"过\" + 0.010*\"会\" + 0.010*\"就\" + 0.009*\"都\" + 0.009*\"多\" + 0.007*\"里\" + 0.007*\"能\" + 0.007*\"要\" + 0.006*\"没\" + 0.005*\"才\" + 0.005*\"真\" + 0.004*\"对\" + 0.004*\"太\" + 0.004*\"梦\" + 0.003*\"像\" + 0.003*\"快乐\" + 0.003*\"说\" + 0.003*\"听\" + 0.003*\"出\" + 0.003*\"啊\" + 0.003*\"走\" + 0.003*\"越\" + 0.003*\"知道\" + 0.003*\"心\" + 0.003*\"样\" + 0.003*\"时候\" + 0.003*\"点\" + 0.003*\"国王\" + 0.003*\"吗\" + 0.002*\"份\" + 0.002*\"怕\" + 0.002*\"等\" + 0.002*\"朋友\" + 0.002*\"满\" + 0.002*\"做\" + 0.002*\"变\" + 0.002*\"地\" + 0.002*\"忘记\" + 0.002*\"找\" + 0.002*\"家\" + 0.002*\"成\" + 0.002*\"世界\" + 0.002*\"烦恼\" + 0.002*\"首\" + 0.002*\"时\" + 0.002*\"手\" + 0.002*\"爱情\" + 0.002*\"经\" + 0.002*\"回\" + 0.002*\"离开\" + 0.002*\"歌\" + 0.002*\"难\" + 0.002*\"天\" + 0.002*\"钱\" + 0.002*\"幸福\" + 0.002*\"时间\" + 0.002*\"心里\" + 0.002*\"眼泪\" + 0.002*\"唱\" + 0.002*\"相\" + 0.002*\"见\" + 0.002*\"明白\" + 0.002*\"中\" + 0.002*\"心情\" + 0.002*\"相信\" + 0.002*\"未\" + 0.002*\"令\" + 0.002*\"十平\" + 0.002*\"坏\" + 0.001*\"美\" + 0.001*\"更\" + 0.001*\"阿\" + 0.001*\"记\" + 0.001*\"写\" + 0.001*\"将\" + 0.001*\"寂寞\" + 0.001*\"心中\" + 0.001*\"温柔\" + 0.001*\"进\" + 0.001*\"留\" + 0.001*\"生命\" + 0.001*\"懂\" + 0.001*\"演唱\" + 0.001*\"己\" + 0.001*\"选择\" + 0.001*\"大\" + 0.001*\"生活\" + 0.001*\"两\" + 0.001*\"话\" + 0.001*\"放弃\" + 0.001*\"快\" + 0.001*\"花\" + 0.001*\"眼睛\" + 0.001*\"秒\" + 0.001*\"拥抱\" + 0.001*\"开心\" + 0.001*\"长段\"'),\n",
       " (3,\n",
       "  '0.167*\" \" + 0.010*\"都\" + 0.010*\"就\" + 0.008*\"爱\" + 0.006*\"要\" + 0.006*\"能\" + 0.005*\"过\" + 0.005*\"想\" + 0.005*\"没\" + 0.004*\"像\" + 0.004*\"会\" + 0.004*\"对\" + 0.004*\"多\" + 0.004*\"说\" + 0.003*\"太\" + 0.003*\"够\" + 0.003*\"地\" + 0.003*\"前\" + 0.003*\"家\" + 0.003*\"走\" + 0.003*\"更\" + 0.003*\"见\" + 0.002*\"感觉\" + 0.002*\"天\" + 0.002*\"中\" + 0.002*\"世界\" + 0.002*\"幸福\" + 0.002*\"未\" + 0.002*\"将\" + 0.002*\"怕\" + 0.002*\"听\" + 0.002*\"啊\" + 0.002*\"等\" + 0.002*\"两\" + 0.002*\"离开\" + 0.002*\"亲爱\" + 0.002*\"知道\" + 0.002*\"唱\" + 0.002*\"大\" + 0.002*\"越\" + 0.002*\"时间\" + 0.002*\"习惯\" + 0.002*\"留\" + 0.002*\"时\" + 0.002*\"美\" + 0.002*\"住\" + 0.002*\"里\" + 0.002*\"愿\" + 0.002*\"身边\" + 0.002*\"算\" + 0.002*\"心\" + 0.001*\"永远\" + 0.001*\"成\" + 0.001*\"飞\" + 0.001*\"正\" + 0.001*\"远\" + 0.001*\"轻轻\" + 0.001*\"梦想\" + 0.001*\"少\" + 0.001*\"多少\" + 0.001*\"开始\" + 0.001*\"呢\" + 0.001*\"望\" + 0.001*\"微笑\" + 0.001*\"己\" + 0.001*\"敢\" + 0.001*\"出\" + 0.001*\"姑娘\" + 0.001*\"满\" + 0.001*\"真\" + 0.001*\"生\" + 0.001*\"忘\" + 0.001*\"改变\" + 0.001*\"绿坝\" + 0.001*\"喜欢\" + 0.001*\"花\" + 0.001*\"做\" + 0.001*\"才\" + 0.001*\"情\" + 0.001*\"笑\" + 0.001*\"乱\" + 0.001*\"早\" + 0.001*\"掉\" + 0.001*\"难\" + 0.001*\"歌\" + 0.001*\"找\" + 0.001*\"阿\" + 0.001*\"生活\" + 0.001*\"快\" + 0.001*\"带\" + 0.001*\"变\" + 0.001*\"梦\" + 0.001*\"时候\" + 0.001*\"处\" + 0.001*\"道光\" + 0.001*\"经\" + 0.001*\"请\" + 0.001*\"黑\" + 0.001*\"慢慢\" + 0.001*\"回忆\"'),\n",
       " (4,\n",
       "  '0.035*\"咯\" + 0.023*\"啊\" + 0.018*\"嘚呔\" + 0.008*\"嘶嘚\" + 0.007*\"嘚\" + 0.006*\"爱\" + 0.006*\"呀\" + 0.006*\"咿\" + 0.006*\"能\" + 0.006*\"想\" + 0.005*\"要\" + 0.005*\"呦\" + 0.005*\" \" + 0.005*\"哦\" + 0.004*\"大王\" + 0.004*\"啲\" + 0.004*\"对\" + 0.004*\"呔嘚\" + 0.004*\"出\" + 0.003*\"没\" + 0.003*\"未\" + 0.003*\"会\" + 0.003*\"永远\" + 0.003*\"世界\" + 0.003*\"吺呔\" + 0.003*\"巡山\" + 0.003*\"路\" + 0.003*\"手\" + 0.003*\"花\" + 0.003*\"叫\" + 0.003*\"时\" + 0.003*\"心中\" + 0.003*\"间\" + 0.003*\"见\" + 0.003*\"就\" + 0.003*\"中\" + 0.003*\"做\" + 0.003*\"哭\" + 0.003*\"水牛\" + 0.003*\"身边\" + 0.002*\"经\" + 0.002*\"山\" + 0.002*\"累\" + 0.002*\"牵\" + 0.002*\"呗\" + 0.002*\"感觉\" + 0.002*\"哈\" + 0.002*\"伤心\" + 0.002*\"达\" + 0.002*\"早\" + 0.002*\"种\" + 0.002*\"诶\" + 0.002*\"呔\" + 0.002*\"吺\" + 0.002*\"脚步\" + 0.002*\"天\" + 0.002*\"全\" + 0.002*\"变成\" + 0.002*\"先\" + 0.002*\"远\" + 0.002*\"遥远\" + 0.002*\"哭哭啼啼\" + 0.002*\"黎明\" + 0.002*\"条\" + 0.002*\"怜化\" + 0.002*\"抛\" + 0.002*\"时代\" + 0.002*\"独占\" + 0.002*\"洒\" + 0.002*\"地方\" + 0.002*\"闻\" + 0.002*\"约定\" + 0.002*\"潇洒\" + 0.002*\"远方\" + 0.002*\"熟悉\" + 0.002*\"演唱\" + 0.001*\"勇敢\" + 0.001*\"都\" + 0.001*\"梦\" + 0.001*\"呔卟\" + 0.001*\"啲呔\" + 0.001*\"必\" + 0.001*\"送\" + 0.001*\"海\" + 0.001*\"地\" + 0.001*\"相\" + 0.001*\"烦想\" + 0.001*\"原野\" + 0.001*\"见求\" + 0.001*\"千古神话\" + 0.001*\"独行\" + 0.001*\"飞\" + 0.001*\"放弃\" + 0.001*\"思念\" + 0.001*\"依恋\" + 0.001*\"愿\" + 0.001*\"疯狂\" + 0.001*\"知道\" + 0.001*\"像\" + 0.001*\"落\"'),\n",
       " (5,\n",
       "  '0.012*\"要\" + 0.011*\"中\" + 0.011*\"都\" + 0.010*\"能\" + 0.009*\"就\" + 0.006*\"里\" + 0.006*\"走\" + 0.006*\"想\" + 0.005*\"世界\" + 0.005*\"过\" + 0.005*\" \" + 0.005*\"见\" + 0.005*\"逢\" + 0.005*\"说\" + 0.005*\"等\" + 0.004*\"会\" + 0.004*\"永远\" + 0.004*\"天\" + 0.004*\"消失\" + 0.004*\"请\" + 0.003*\"像\" + 0.003*\"离开\" + 0.003*\"梦\" + 0.003*\"相手\" + 0.003*\"切\" + 0.003*\"希望\" + 0.003*\"爱\" + 0.003*\"蓝天\" + 0.003*\"爱情\" + 0.003*\"家\" + 0.002*\"知道\" + 0.002*\"熟悉\" + 0.002*\"没\" + 0.002*\"才\" + 0.002*\"太\" + 0.002*\"种\" + 0.002*\"生命\" + 0.002*\"找\" + 0.002*\"眼带\" + 0.002*\"穿越\" + 0.002*\"渐渐\" + 0.002*\"红\" + 0.002*\"拥抱\" + 0.002*\"分离\" + 0.002*\"坚强\" + 0.002*\"变\" + 0.002*\"经\" + 0.002*\"记忆\" + 0.002*\"风\" + 0.002*\"坏孩\" + 0.002*\"歌唱\" + 0.002*\"陌生\" + 0.002*\"眼睛\" + 0.002*\"带\" + 0.002*\"己\" + 0.002*\"多\" + 0.002*\"真\" + 0.002*\"十\" + 0.002*\"眼前\" + 0.002*\"真夜\" + 0.002*\"摇摆\" + 0.002*\"城市\" + 0.002*\"情歌\" + 0.002*\"对\" + 0.002*\"前\" + 0.002*\"女\" + 0.002*\"天空\" + 0.001*\"乡\" + 0.001*\"年\" + 0.001*\"更\" + 0.001*\"牵\" + 0.001*\"必\" + 0.001*\"想想\" + 0.001*\"第眼\" + 0.001*\"誓言\" + 0.001*\"动物园\" + 0.001*\"梦见\" + 0.001*\"桃花\" + 0.001*\"甘爱\" + 0.001*\"岗\" + 0.001*\"辛苦\" + 0.001*\"生路\" + 0.001*\"日报\" + 0.001*\"改变\" + 0.001*\"走过\" + 0.001*\"感觉\" + 0.001*\"演唱\" + 0.001*\"做\" + 0.001*\"开始\" + 0.001*\"慢慢\" + 0.001*\"次\" + 0.001*\"怕\" + 0.001*\"时候\" + 0.001*\"伤心\" + 0.001*\"边\" + 0.001*\"游子\" + 0.001*\"热血\" + 0.001*\"细雨\" + 0.001*\"生世\" + 0.001*\"注定\"'),\n",
       " (6,\n",
       "  '0.020*\" \" + 0.016*\"没\" + 0.016*\"要\" + 0.009*\"爱\" + 0.008*\"说\" + 0.008*\"都\" + 0.008*\"过\" + 0.008*\"知道\" + 0.007*\"想\" + 0.007*\"里\" + 0.007*\"能\" + 0.006*\"就\" + 0.005*\"多\" + 0.005*\"中\" + 0.005*\"己\" + 0.005*\"会\" + 0.005*\"经\" + 0.005*\"对\" + 0.004*\"唱\" + 0.004*\"像\" + 0.004*\"前\" + 0.003*\"走\" + 0.003*\"忘\" + 0.003*\"出\" + 0.003*\"梦\" + 0.003*\"告诉\" + 0.003*\"首\" + 0.003*\"才\" + 0.003*\"永远\" + 0.003*\"早\" + 0.003*\"大\" + 0.002*\"心中\" + 0.002*\"做\" + 0.002*\"哎\" + 0.002*\"歌\" + 0.002*\"忘记\" + 0.002*\"停泊\" + 0.002*\"情绪\" + 0.002*\"必\" + 0.002*\"哭泣\" + 0.002*\"啊\" + 0.002*\"轻轻\" + 0.002*\"留\" + 0.002*\"听\" + 0.002*\"陪伴\" + 0.002*\"望\" + 0.002*\"问\" + 0.002*\"等\" + 0.002*\"变\" + 0.002*\"演唱\" + 0.002*\"太\" + 0.002*\"世界\" + 0.002*\"两\" + 0.002*\"见\" + 0.002*\"回头\" + 0.002*\"开心\" + 0.002*\"能够\" + 0.002*\"春天\" + 0.002*\"明明\" + 0.002*\"路\" + 0.002*\"寂寞\" + 0.002*\"青春\" + 0.002*\"愿\" + 0.002*\"远行\" + 0.002*\"飞\" + 0.002*\"回忆\" + 0.001*\"疼\" + 0.001*\"更\" + 0.001*\"等待\" + 0.001*\"徘徊\" + 0.001*\"应\" + 0.001*\"许多\" + 0.001*\"难\" + 0.001*\"明星\" + 0.001*\"场\" + 0.001*\"美丽\" + 0.001*\"努力\" + 0.001*\"相信\" + 0.001*\"听见\" + 0.001*\"将\" + 0.001*\"开始\" + 0.001*\"找\" + 0.001*\"停\" + 0.001*\"烦恼\" + 0.001*\"城市\" + 0.001*\"意义\" + 0.001*\"装\" + 0.001*\"似\" + 0.001*\"难道\" + 0.001*\"重\" + 0.001*\"回\" + 0.001*\"成\" + 0.001*\"拥抱\" + 0.001*\"脚步\" + 0.001*\"地\" + 0.001*\"生命\" + 0.001*\"生活\" + 0.001*\"就算\" + 0.001*\"家\" + 0.001*\"越\"'),\n",
       " (7,\n",
       "  '0.033*\"爱\" + 0.030*\" \" + 0.021*\"要\" + 0.012*\"想\" + 0.012*\"说\" + 0.012*\"没\" + 0.012*\"就\" + 0.010*\"过\" + 0.008*\"能\" + 0.008*\"都\" + 0.007*\"会\" + 0.007*\"家\" + 0.005*\"对\" + 0.005*\"里\" + 0.005*\"才\" + 0.004*\"多\" + 0.004*\"太\" + 0.004*\"走\" + 0.004*\"开\" + 0.004*\"明白\" + 0.003*\"越\" + 0.003*\"总\" + 0.003*\"难\" + 0.003*\"离开\" + 0.003*\"真\" + 0.003*\"等\" + 0.003*\"爱情\" + 0.003*\"幸福\" + 0.003*\"懂\" + 0.003*\"手\" + 0.003*\"多少\" + 0.003*\"回\" + 0.003*\"放\" + 0.003*\"问\" + 0.002*\"己\" + 0.002*\"出\" + 0.002*\"做\" + 0.002*\"美\" + 0.002*\"孩子\" + 0.002*\"世界\" + 0.002*\"花\" + 0.002*\"回忆\" + 0.002*\"怕\" + 0.002*\"年\" + 0.002*\"陪\" + 0.002*\"演唱\" + 0.002*\"耶\" + 0.002*\"长\" + 0.002*\"哭\" + 0.002*\"心里\" + 0.002*\"够\" + 0.002*\"哩\" + 0.002*\"哒咚\" + 0.002*\"美丽\" + 0.002*\"请\" + 0.002*\"就算\" + 0.002*\"留\" + 0.002*\"两\" + 0.002*\"中\" + 0.002*\"离\" + 0.002*\"天\" + 0.002*\"便\" + 0.002*\"经\" + 0.002*\"感觉\" + 0.002*\"快乐\" + 0.002*\"心\" + 0.002*\"决定\" + 0.002*\"否\" + 0.002*\"啊\" + 0.002*\"知道\" + 0.002*\"难过\" + 0.002*\"温暖\" + 0.002*\"坏\" + 0.002*\"眼泪\" + 0.002*\"前\" + 0.002*\"风\" + 0.002*\"地\" + 0.002*\"时\" + 0.002*\"天空\" + 0.002*\"永远\" + 0.002*\"眼睛\" + 0.002*\"嗒滴\" + 0.002*\"像\" + 0.002*\"走过\" + 0.001*\"梦\" + 0.001*\"寂寞\" + 0.001*\"错\" + 0.001*\"开放\" + 0.001*\"种\" + 0.001*\"见\" + 0.001*\"更\" + 0.001*\"依然\" + 0.001*\"呀\" + 0.001*\"生活\" + 0.001*\"必\" + 0.001*\"等待\" + 0.001*\"时候\" + 0.001*\"深\" + 0.001*\"天堂\" + 0.001*\"原谅\"'),\n",
       " (8,\n",
       "  '0.025*\" \" + 0.013*\"想\" + 0.011*\"爱\" + 0.010*\"会\" + 0.009*\"要\" + 0.008*\"里\" + 0.007*\"就\" + 0.007*\"呀\" + 0.006*\"没\" + 0.006*\"都\" + 0.006*\"过\" + 0.006*\"太\" + 0.005*\"啊\" + 0.005*\"阿\" + 0.005*\"多\" + 0.005*\"天\" + 0.004*\"难\" + 0.004*\"唱\" + 0.004*\"能\" + 0.004*\"思念\" + 0.004*\"越\" + 0.004*\"永远\" + 0.004*\"中\" + 0.004*\"否\" + 0.004*\"雪\" + 0.004*\"爱情\" + 0.004*\"飞\" + 0.004*\"吗\" + 0.004*\"说\" + 0.004*\"歌\" + 0.003*\"嘞\" + 0.003*\"幸福\" + 0.003*\"梦\" + 0.003*\"寂寞\" + 0.003*\"时\" + 0.003*\"像\" + 0.003*\"世界\" + 0.003*\"见\" + 0.003*\"首\" + 0.003*\"知道\" + 0.003*\"经\" + 0.003*\"将\" + 0.003*\"时候\" + 0.002*\"对\" + 0.002*\"落\" + 0.002*\"更\" + 0.002*\"总\" + 0.002*\"才\" + 0.002*\"雨\" + 0.002*\"出\" + 0.002*\"未\" + 0.002*\"狠心\" + 0.002*\"等\" + 0.002*\"带\" + 0.002*\"回忆\" + 0.002*\"快乐\" + 0.002*\"学会\" + 0.002*\"心情\" + 0.002*\"忘\" + 0.002*\"明白\" + 0.002*\"心\" + 0.002*\"美丽\" + 0.002*\"伤心\" + 0.002*\"事\" + 0.002*\"美\" + 0.002*\"早\" + 0.002*\"大\" + 0.002*\"找\" + 0.002*\"时光\" + 0.002*\"风\" + 0.002*\"眼睛\" + 0.002*\"陪\" + 0.002*\"走\" + 0.002*\"夜\" + 0.002*\"问题\" + 0.002*\"全部\" + 0.002*\"放纵\" + 0.002*\"大片\" + 0.002*\"甜\" + 0.002*\"疯狂\" + 0.002*\"选择\" + 0.001*\"飘\" + 0.001*\"演唱\" + 0.001*\"懂\" + 0.001*\"共\" + 0.001*\"前\" + 0.001*\"放\" + 0.001*\"记忆\" + 0.001*\"真\" + 0.001*\"日子\" + 0.001*\"等待\" + 0.001*\"地\" + 0.001*\"滋味\" + 0.001*\"分\" + 0.001*\"动生心动\" + 0.001*\"探戈\" + 0.001*\"眼泪\" + 0.001*\"笑\" + 0.001*\"悄悄\" + 0.001*\"颜色\"'),\n",
       " (9,\n",
       "  '0.022*\" \" + 0.019*\"都\" + 0.016*\"说\" + 0.016*\"多\" + 0.007*\"要\" + 0.007*\"想\" + 0.007*\"爱\" + 0.006*\"时间\" + 0.006*\"对\" + 0.006*\"必\" + 0.005*\"太\" + 0.005*\"梦\" + 0.005*\"中\" + 0.004*\"知道\" + 0.004*\"会\" + 0.004*\"像\" + 0.004*\"过\" + 0.004*\"天\" + 0.003*\"里\" + 0.003*\"真\" + 0.003*\"希望\" + 0.003*\"没\" + 0.003*\"什\" + 0.003*\"爱情\" + 0.003*\"就\" + 0.003*\"草原\" + 0.003*\"风\" + 0.003*\"身边\" + 0.003*\"时候\" + 0.003*\"追忆\" + 0.003*\"做\" + 0.003*\"变成\" + 0.002*\"快乐\" + 0.002*\"心\" + 0.002*\"世界\" + 0.002*\"许愿\" + 0.002*\"圈圈\" + 0.002*\"大\" + 0.002*\"感觉\" + 0.002*\"更\" + 0.002*\"黑\" + 0.002*\"等\" + 0.002*\"现实\" + 0.002*\"寻找\" + 0.002*\"庆祝\" + 0.002*\"祝福\" + 0.002*\"乎\" + 0.002*\"生日\" + 0.002*\"值\" + 0.002*\"懂\" + 0.002*\"灰\" + 0.002*\"将\" + 0.002*\"地\" + 0.002*\"嘴\" + 0.002*\"目光\" + 0.002*\"盏\" + 0.002*\"珍珠\" + 0.002*\"蜡烛\" + 0.002*\"灯\" + 0.002*\"颗泪\" + 0.002*\"开\" + 0.002*\"敢\" + 0.002*\"慢慢\" + 0.002*\"浪漫\" + 0.002*\"失控\" + 0.002*\"思念\" + 0.001*\"记\" + 0.001*\"连\" + 0.001*\"梦近\" + 0.001*\"流浪\" + 0.001*\"追求者\" + 0.001*\"渺中\" + 0.001*\"原野\" + 0.001*\"太阳\" + 0.001*\"马汉子\" + 0.001*\"换\" + 0.001*\"望际\" + 0.001*\"心底\" + 0.001*\"伤然\" + 0.001*\"加深\" + 0.001*\"刻意\" + 0.001*\"应\" + 0.001*\"模仿样\" + 0.001*\"妈妈\" + 0.001*\"尝试\" + 0.001*\"觉\" + 0.001*\"接\" + 0.001*\"啊\" + 0.001*\"能\" + 0.001*\"颜色\" + 0.001*\"离\" + 0.001*\"见\" + 0.001*\"带\" + 0.001*\"出\" + 0.001*\"藏\" + 0.001*\"开始\" + 0.001*\"前\" + 0.001*\"模样\" + 0.001*\"才\" + 0.001*\"点\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=10, num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# original version\n",
    "def find_topic(texts, topic_model, n_topics, vec_model=\"tf\", thr=1e-2, **kwargs):\n",
    "        # 1. vectorization\n",
    "        vectorizer = CountVectorizer() if vec_model== \"tf\" else TfidfVectorizer()\n",
    "        text_vec = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        words = np.array(vectorizer.get_feature_names())\n",
    "        return words\n",
    "        # 2. topic finding\n",
    "        topic_models = {\"nmf\":NMF, \"svd\": TruncatedSVD, \"lda\":LatentDirichletAllocation, \"kmeans\":KMeans}\n",
    "        topicfinder = topic_models[topic_model](n_topics, **kwargs).fit(text_vec)\n",
    "        topic_dists = topicfinder.components_ if topic_model is not \"kmeans\" else topicfinder.cluster_centers_\n",
    "        topic_dists /= topic_dists.max(axis = 1).reshape((-1,1))\n",
    "        \n",
    "        # 3. keywords for topics\n",
    "        def _topic_keywords(topic_dist):\n",
    "            keywords_index = np.abs(topic_dist) >= thr\n",
    "            keywords_prefix = np.where(np.sign(topic_dist)>0, \"\",\"^\")[keywords_index]\n",
    "            keywords = \" | \".join(map(lambda x: \"\".join(x), zip(keywords_prefix, words[keywords_index])))\n",
    "            return keywords\n",
    "        topic_keywords = map(_topic_keywords, topic_dists)\n",
    "        return \"\\n\".join(\"Topic %i:%s\" % (i, t) for i, t in enumerate(topic_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:之间 | 从前 | 回忆 | 彼此 | 忘记 | 慢慢 | 我们 | 拥有 | 故事 | 无法 | 明天 | 曾经 | 最后 | 朋友 | 相爱 | 相遇 | 祝福 | 约定 | 记得\n",
      "Topic 1:关系 | 其实 | 发现 | 后悔 | 地方 | 感觉 | 改变 | 时候 | 没有 | 烦恼 | 理由 | 虽然 | 身边\n",
      "Topic 2:一个 | 习惯 | 只是 | 地方 | 孤单 | 拥抱 | 最后 | 朋友 | 梦想 | 爱上 | 理由 | 生活 | 眼神 | 等待 | 角落 | 遇见\n",
      "Topic 3:一切 | 其实 | 已经 | 心中 | 时候 | 现在 | 知道 | 需要\n",
      "Topic 4:勇气 | 发现 | 只能 | 可以 | 告诉 | 悲伤 | 改变 | 放纵 | 无法 | 明白 | 最后 | 欺骗 | 相信 | 美丽 | 自己 | 觉得 | 迷失 | 面对\n",
      "Topic 5:分开 | 分离 | 就是 | 心中 | 忘记 | 怀念 | 放弃 | 朋友 | 永远 | 直到 | 相信 | 等待 | 美丽 | 身边\n",
      "Topic 6:世界 | 从此 | 只有 | 只要 | 尽头 | 属于 | 改变 | 整个 | 美丽 | 身边 | 这个\n",
      "Topic 7:友情 | 可以 | 失去 | 已经 | 故事 | 曾经 | 游戏 | 爱情 | 甜蜜 | 相信 | 眼泪 | 美丽\n",
      "Topic 8:一定 | 一生 | 只要 | 存在 | 安康 | 幸福 | 心中 | 感到 | 感觉 | 明天 | 欢迎 | 每天 | 浪漫 | 滋味 | 甜蜜 | 生活 | 痛苦 | 相信 | 美好\n",
      "Topic 9:一起 | 何必 | 兄弟 | 可以 | 好好 | 时候 | 最后 | 朋友 | 白头 | 记得 | 走过 | 跟着 | 跳舞 | 这里\n",
      "Topic 10:一切 | 今天 | 从前 | 其实 | 可以 | 听到 | 告诉 | 喜欢 | 回到 | 回忆 | 失去 | 好好 | 如果 | 就是 | 已经 | 得到 | 忘记 | 愿意 | 拥有 | 时间 | 明天 | 爱上 | 爱人 | 生命 | 相信 | 真心 | 眼泪 | 继续 | 能够 | 还有 | 遇见 | 重头\n",
      "Topic 11:一生 | 一直 | 下去 | 为何 | 以为 | 伤心 | 分手 | 可以 | 告诉 | 喜欢 | 多少 | 如何 | 就是 | 总是 | 折磨 | 温柔 | 爱上 | 现在 | 生活 | 男人 | 结局 | 结果 | 缠绵 | 过去 | 这样 | 难道 | 面对 | 默默\n",
      "Topic 12:一生 | 只是 | 只有 | 味道 | 多少 | 夜里 | 失去 | 孤单 | 孤独 | 害怕 | 寂寞 | 思念 | 情歌 | 想起 | 时候 | 明白 | 曾经 | 温暖 | 温柔 | 相信 | 等待\n",
      "Topic 13:一生 | 伤心 | 健康 | 其实 | 只要 | 天天 | 希望 | 开心 | 心中 | 快乐 | 悲伤 | 感觉 | 所以 | 所有 | 曾经 | 现在 | 生日 | 生活 | 简单 | 轻松 | 过去 | 零点\n",
      "Topic 14:一切 | 习惯 | 勇敢 | 只是 | 因为 | 在乎 | 失去 | 女人 | 害怕 | 已经 | 年轻 | 所以 | 爱上 | 爱所以 | 相信 | 美丽 | 身边 | 这里 | 难过 | 骄傲\n",
      "Topic 15:一切 | 一样 | 一直 | 为何 | 以后 | 依然 | 再见 | 到底 | 喜欢 | 回头 | 忘记 | 想起 | 明白 | 最后 | 现在 | 相信 | 等待 | 虽然 | 还是 | 这里 | 选择 | 那个\n",
      "Topic 16:一刻 | 为何 | 亲爱 | 以后 | 再见 | 决定 | 回到 | 回头 | 就要 | 已经 | 开始 | 微笑 | 感觉 | 所以 | 拥抱 | 时候 | 时间 | 最后 | 独自 | 现在 | 离开 | 能够 | 赤裸裸 | 身旁 | 身边 | 选择 | 遇见 | 面对\n",
      "Topic 17:一切 | 一直 | 为什 | 亲爱 | 今天 | 出现 | 别人 | 到底 | 忘记 | 总是 | 时间 | 明白 | 曾经 | 流泪 | 生活 | 相信 | 相爱 | 眼泪 | 究竟 | 能够 | 说话 | 难道\n",
      "Topic 18:一生 | 一直 | 他们 | 伤悲 | 依然 | 偷偷 | 其实 | 只有 | 回忆 | 声音 | 多少 | 失去 | 彼此 | 徘徊 | 心里 | 忘记 | 总是 | 感觉 | 慢慢 | 无限 | 明白 | 暖暖 | 最后 | 朋友 | 歌声 | 温暖 | 温柔 | 留下 | 相爱 | 眼泪 | 美丽 | 过去 | 选择 | 难过 | 默默\n",
      "Topic 19:冷漠 | 演唱 | 百合二 | 郑少秋\n"
     ]
    }
   ],
   "source": [
    "# vec model is tf\n",
    "\n",
    "# print(find_topic(bag,\"svd\",20, vec_model = \"tf\"))\n",
    "# print(find_topic(bag,\"nmf\",20, vec_model = \"tf\"))\n",
    "# print(find_topic(bag,\"lda\",20, vec_model = \"tf\"))\n",
    "# print(find_topic(bag,\"kmeans\",20, vec_model = \"tf\"))\n",
    "\n",
    "# vec model is tfidf\n",
    "\n",
    "#print(find_topic(bag,\"svd\",20, vec_model = \"tfidf\"))\n",
    "# print(find_topic(bag, \"nmf\", 20, vec_model = \"tfidf\"))\n",
    "#print(find_topic(bag,\"lda\",20, vec_model = \"tfidf\"))\n",
    "# print(find_topic(bag,\"kmeans\",20, vec_model = \"tfidf\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cqb]",
   "language": "python",
   "name": "conda-env-cqb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
